{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPw5gasVr3VCHUo/v7PQmjV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coldbilberry/repo-gui/blob/main/ML_in_Business_HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1."
      ],
      "metadata": {
        "id": "tVIybaEin4V1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thM2O8Ffk9Eq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "#from nltk.tokenize import word_tokenize\n",
        "\n",
        "from razdel import tokenize # https://github.com/natasha/razdel\n",
        "\n",
        "import pymorphy2\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, roc_auc_score, precision_score, classification_report, \\\n",
        "                                precision_recall_curve, confusion_matrix\n",
        "import itertools\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "news = pd.read_csv(\"articles.csv\")\n",
        "print(news.shape)\n",
        "news.head(3)"
      ],
      "metadata": {
        "id": "KgtlXjhQmRlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users = pd.read_csv(\"users_articles.csv\")\n",
        "users.head(3)"
      ],
      "metadata": {
        "id": "ErydFP0smWQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import nltk\n",
        "#nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "QAjods2dmY5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopword_ru = stopwords.words('russian')\n",
        "len(stopword_ru)\n",
        "morph = pymorphy2.MorphAnalyzer()"
      ],
      "metadata": {
        "id": "U_yB4qmRma14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('stopwords.txt') as f:\n",
        "    additional_stopwords = [w.strip() for w in f.readlines() if w]\n",
        "stopword_ru += additional_stopwords\n",
        "len(stopword_ru)"
      ],
      "metadata": {
        "id": "gc4NOlTpmdRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    '''\n",
        "    очистка текста\n",
        "\n",
        "    на выходе очищеный текст\n",
        "\n",
        "    '''\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "\n",
        "    text = text.lower()\n",
        "    text = text.strip('\\n').strip('\\r').strip('\\t')\n",
        "    text = re.sub(\"-\\s\\r\\n\\|-\\s\\r\\n|\\r\\n\", '', str(text))\n",
        "\n",
        "    text = re.sub(\"[0-9]|[-—.,:;_%©«»?*!@#№$^•·&()]|[+=]|[[]|[]]|[/]|\", '', text)\n",
        "    text = re.sub(r\"\\r\\n\\t|\\n|\\\\s|\\r\\t|\\\\n\", ' ', text)\n",
        "    text = re.sub(r'[\\xad]|[\\s+]', ' ', text.strip())\n",
        "\n",
        "    #tokens = list(tokenize(text))\n",
        "    #words = [_.text for _ in tokens]\n",
        "    #words = [w for w in words if w not in stopword_ru]\n",
        "\n",
        "    #return \" \".join(words)\n",
        "    return text\n",
        "\n",
        "cache = {}\n",
        "\n",
        "def lemmatization(text):\n",
        "    '''\n",
        "    лемматизация\n",
        "        [0] если зашел тип не `str` делаем его `str`\n",
        "        [1] токенизация предложения через razdel\n",
        "        [2] проверка есть ли в начале слова '-'\n",
        "        [3] проверка токена с одного символа\n",
        "        [4] проверка есть ли данное слово в кэше\n",
        "        [5] лемматизация слова\n",
        "        [6] проверка на стоп-слова\n",
        "\n",
        "    на выходе лист отлемматизированых токенов\n",
        "    '''\n",
        "\n",
        "    # [0]\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "\n",
        "    # [1]\n",
        "    tokens = list(tokenize(text))\n",
        "    words = [_.text for _ in tokens]\n",
        "\n",
        "    words_lem = []\n",
        "    for w in words:\n",
        "        if w[0] == '-': # [2]\n",
        "            w = w[1:]\n",
        "        if len(w)>1: # [3]\n",
        "            if w in cache: # [4]\n",
        "                words_lem.append(cache[w])\n",
        "            else: # [5]\n",
        "                temp_cach = cache[w] = morph.parse(w)[0].normal_form\n",
        "                words_lem.append(temp_cach)\n",
        "\n",
        "    words_lem_without_stopwords=[i for i in words_lem if not i in stopword_ru] # [6]\n",
        "\n",
        "    return words_lem_without_stopwords"
      ],
      "metadata": {
        "id": "Tu7ZIaMPmhL7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "news['title'] = news['title'].apply(lambda x: clean_text(x), 1)"
      ],
      "metadata": {
        "id": "9npR3MsLmjoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "news['title'] = news['title'].apply(lambda x: lemmatization(x), 1)"
      ],
      "metadata": {
        "id": "NHPlH8--mqdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [t for t in news['title'].values]\n",
        "\n",
        "# Create a corpus from a list of texts\n",
        "common_dictionary = Dictionary(texts)\n",
        "common_corpus = [common_dictionary.doc2bow(text) for text in texts]"
      ],
      "metadata": {
        "id": "Fzmzs6pXmyPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_dictionary[10]"
      ],
      "metadata": {
        "id": "k43MMnz1m0z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from gensim.models import LdaModel\n",
        "# Train the model on the corpus.\n",
        "lda = LdaModel(common_corpus, num_topics=25, id2word=common_dictionary)#, passes=10)"
      ],
      "metadata": {
        "id": "0ANHrBRSm4Gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.test.utils import datapath\n",
        "# Save model to disk.\n",
        "temp_file = datapath(\"model.lda\")\n",
        "lda.save(temp_file)\n",
        "\n",
        "# Load a potentially pretrained model from disk.\n",
        "lda = LdaModel.load(temp_file)"
      ],
      "metadata": {
        "id": "JCal4NH_m6O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new corpus, made of previously unseen documents.\n",
        "other_texts = [t for t in news['title'].iloc[:3]]\n",
        "other_corpus = [common_dictionary.doc2bow(text) for text in other_texts]\n",
        "\n",
        "unseen_doc = other_corpus[2]\n",
        "print(other_texts[2])\n",
        "lda[unseen_doc]"
      ],
      "metadata": {
        "id": "o74tpMILm8Mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=lda.show_topics(num_topics=25, num_words=7,formatted=False)\n",
        "topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in x]\n",
        "\n",
        "#Below Code Prints Only Words\n",
        "for topic,words in topics_words:\n",
        "    print(\"topic_{}: \".format(topic)+\" \".join(words))"
      ],
      "metadata": {
        "id": "xQcYFdyOm8Qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#text = news['title'].iloc[0]\n",
        "\n",
        "def get_lda_vector(text):\n",
        "    unseen_doc = common_dictionary.doc2bow(text)\n",
        "    lda_tuple = lda[unseen_doc]\n",
        "    not_null_topics = dict(zip([i[0] for i in lda_tuple], [i[1] for i in lda_tuple]))\n",
        "\n",
        "    output_vector = []\n",
        "    for i in range(25):\n",
        "        if i not in not_null_topics:\n",
        "            output_vector.append(0)\n",
        "        else:\n",
        "            output_vector.append(not_null_topics[i])\n",
        "    return np.array(output_vector)"
      ],
      "metadata": {
        "id": "_-txSiHMnB8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_matrix = pd.DataFrame([get_lda_vector(text) for text in news['title'].values])\n",
        "topic_matrix.columns = ['topic_{}'.format(i) for i in range(25)]\n",
        "topic_matrix['doc_id'] = news['doc_id'].values\n",
        "topic_matrix = topic_matrix[['doc_id']+['topic_{}'.format(i) for i in range(25)]]\n",
        "topic_matrix.head(5)"
      ],
      "metadata": {
        "id": "nJHvvMFjnFpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users.head(3)"
      ],
      "metadata": {
        "id": "QeEYTnrCnHsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_dict = dict(zip(topic_matrix['doc_id'].values, topic_matrix[['topic_{}'.format(i) for i in range(25)]].values))"
      ],
      "metadata": {
        "id": "oL5pc8rjnJwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_dict[293622]"
      ],
      "metadata": {
        "id": "4D3AZLIAnMJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_articles_list = users['articles'].iloc[33]"
      ],
      "metadata": {
        "id": "mRatJVsunOQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_embedding(user_articles_list):\n",
        "    user_articles_list = eval(user_articles_list)\n",
        "    user_vector = np.array([doc_dict[doc_id] for doc_id in user_articles_list])\n",
        "    user_vector = np.mean(user_vector, 0)\n",
        "    return user_vector"
      ],
      "metadata": {
        "id": "WOvKon6pnQqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_user_embedding(user_articles_list)"
      ],
      "metadata": {
        "id": "hF84pMmanS2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users['articles'].iloc[33]"
      ],
      "metadata": {
        "id": "oIm_8lVNnU0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\" \".join(news[news['doc_id']==323186]['title'].iloc[0])"
      ],
      "metadata": {
        "id": "t1lQysmSnW73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_embeddings = pd.DataFrame([i for i in users['articles'].apply(lambda x: get_user_embedding(x), 1)])\n",
        "user_embeddings.columns = ['topic_{}'.format(i) for i in range(25)]\n",
        "user_embeddings['uid'] = users['uid'].values\n",
        "user_embeddings = user_embeddings[['uid']+['topic_{}'.format(i) for i in range(25)]]\n",
        "user_embeddings.head(3)"
      ],
      "metadata": {
        "id": "YFb28xvAnZAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = pd.read_csv(\"users_churn.csv\")\n",
        "target.head(3)"
      ],
      "metadata": {
        "id": "L5FDnKLDnblK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = pd.merge(user_embeddings, target, 'left')\n",
        "X.head(3)"
      ],
      "metadata": {
        "id": "ziqhVOEundtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X[['topic_{}'.format(i) for i in range(25)]],\n",
        "                                                    X['churn'], random_state=0)"
      ],
      "metadata": {
        "id": "ELxKfn6Rng69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "SjsBPtIeniTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = logreg.predict_proba(X_test)[:, 1]\n",
        "preds[:10]"
      ],
      "metadata": {
        "id": "3tH__yF0nlym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision, recall, thresholds = precision_recall_curve(y_test, preds)\n",
        "fscore = (2 * precision * recall) / (precision + recall + 0.0000000001)\n",
        "# locate the index of the largest f score\n",
        "ix = np.argmax(fscore)\n",
        "print('Best Threshold=%f, F-Score=%.3f, Precision=%.3f, Recall=%.3f' % (thresholds[ix],\n",
        "                                                                        fscore[ix],\n",
        "                                                                        precision[ix],\n",
        "                                                                        recall[ix]))"
      ],
      "metadata": {
        "id": "vFBhWIGLnoWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "metadata": {
        "id": "rEa469lEntIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "font = {'size' : 15}\n",
        "\n",
        "plt.rc('font', **font)\n",
        "\n",
        "cnf_matrix = confusion_matrix(y_test, preds>thresholds[ix])\n",
        "plt.figure(figsize=(10, 8))\n",
        "plot_confusion_matrix(cnf_matrix, classes=['Non-Churn', 'churn'],\n",
        "                      title='Confusion matrix')\n",
        "plt.savefig(\"conf_matrix_2.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KUb6o2mAnvSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roc_auc_score(y_test, preds)"
      ],
      "metadata": {
        "id": "lIsoV0A5nxMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2."
      ],
      "metadata": {
        "id": "VfpAOw_wn9oK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_embedding(user_articles_list):\n",
        "    user_articles_list = eval(user_articles_list)\n",
        "    user_vector = np.array([doc_dict[doc_id] for doc_id in user_articles_list])\n",
        "    user_vector = np.median(user_vector, 0)\n",
        "    return user_vector"
      ],
      "metadata": {
        "id": "QdrkKsbyoGim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_user_embedding(user_articles_list)"
      ],
      "metadata": {
        "id": "flVthiIwoJAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_embeddings = pd.DataFrame([i for i in users['articles'].apply(lambda x: get_user_embedding(x), 1)])\n",
        "user_embeddings.columns = ['topic_{}'.format(i) for i in range(25)]\n",
        "user_embeddings['uid'] = users['uid'].values\n",
        "user_embeddings = user_embeddings[['uid']+['topic_{}'.format(i) for i in range(25)]]\n",
        "user_embeddings.head(3)"
      ],
      "metadata": {
        "id": "wte15t51oML1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = pd.merge(user_embeddings, target, 'left')\n",
        "X.head(3)"
      ],
      "metadata": {
        "id": "rig6MwKVoOC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X[['topic_{}'.format(i) for i in range(25)]],\n",
        "                                                    X['churn'], random_state=0)"
      ],
      "metadata": {
        "id": "VCz_9MY7oQaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logreg = LogisticRegression()\n",
        "#обучим\n",
        "logreg.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "z2HQdFtDoSMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3ruvaAxMoUnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = logreg.predict_proba(X_test)[:, 1]\n",
        "preds[:10]"
      ],
      "metadata": {
        "id": "fhUjD5v0oUsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision, recall, thresholds = precision_recall_curve(y_test, preds)\n",
        "fscore = (2 * precision * recall) / (precision + recall + 0.0000000001)\n",
        "# locate the index of the largest f score\n",
        "ix = np.argmax(fscore)\n",
        "print('Best Threshold=%f, F-Score=%.3f, Precision=%.3f, Recall=%.3f' % (thresholds[ix],\n",
        "                                                                        fscore[ix],\n",
        "                                                                        precision[ix],\n",
        "                                                                        recall[ix]))"
      ],
      "metadata": {
        "id": "GClLcpmvoWkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "font = {'size' : 15}\n",
        "\n",
        "plt.rc('font', **font)\n",
        "\n",
        "cnf_matrix = confusion_matrix(y_test, preds>thresholds[ix])\n",
        "plt.figure(figsize=(10, 8))\n",
        "plot_confusion_matrix(cnf_matrix, classes=['Non-Churn', 'churn'],\n",
        "                      title='Confusion matrix')\n",
        "plt.savefig(\"conf_matrix_2.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zlb3Z69hoZOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roc_auc_score(y_test, preds)"
      ],
      "metadata": {
        "id": "w4eU8Cd3obhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_embedding(user_articles_list):\n",
        "    user_articles_list = eval(user_articles_list)\n",
        "    user_vector = np.array([doc_dict[doc_id] for doc_id in user_articles_list])\n",
        "    user_vector = np.max(user_vector, 0)\n",
        "    return user_vector"
      ],
      "metadata": {
        "id": "Ra_KrUEHodTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_user_embedding(user_articles_list)"
      ],
      "metadata": {
        "id": "ZpDVVwIzofDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_embeddings = pd.DataFrame([i for i in users['articles'].apply(lambda x: get_user_embedding(x), 1)])\n",
        "user_embeddings.columns = ['topic_{}'.format(i) for i in range(25)]\n",
        "user_embeddings['uid'] = users['uid'].values\n",
        "user_embeddings = user_embeddings[['uid']+['topic_{}'.format(i) for i in range(25)]]\n",
        "user_embeddings.head(3)"
      ],
      "metadata": {
        "id": "zgCn2o3tohAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = pd.merge(user_embeddings, target, 'left')\n",
        "X.head(3)"
      ],
      "metadata": {
        "id": "s0Vj4YrOojOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X[['topic_{}'.format(i) for i in range(25)]],\n",
        "                                                    X['churn'], random_state=0)"
      ],
      "metadata": {
        "id": "u1meuym1olK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "98LUJV3con98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = logreg.predict_proba(X_test)[:, 1]\n",
        "preds[:10]"
      ],
      "metadata": {
        "id": "lSX2CVXvoqGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision, recall, thresholds = precision_recall_curve(y_test, preds)\n",
        "fscore = (2 * precision * recall) / (precision + recall + 0.0000000001)\n",
        "# locate the index of the largest f score\n",
        "ix = np.argmax(fscore)\n",
        "print('Best Threshold=%f, F-Score=%.3f, Precision=%.3f, Recall=%.3f' % (thresholds[ix],\n",
        "                                                                        fscore[ix],\n",
        "                                                                        precision[ix],\n",
        "                                                                        recall[ix]))"
      ],
      "metadata": {
        "id": "3KJQuYnFouIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "font = {'size' : 15}\n",
        "\n",
        "plt.rc('font', **font)\n",
        "\n",
        "cnf_matrix = confusion_matrix(y_test, preds>thresholds[ix])\n",
        "plt.figure(figsize=(10, 8))\n",
        "plot_confusion_matrix(cnf_matrix, classes=['Non-Churn', 'churn'],\n",
        "                      title='Confusion matrix')\n",
        "plt.savefig(\"conf_matrix_2.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bR1lfXF4owe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roc_auc_score(y_test, preds)"
      ],
      "metadata": {
        "id": "sBa3_unCo5a-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4"
      ],
      "metadata": {
        "id": "5r-tJdcfo6jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(columns=['model', 'roc_auc', 'precision', 'f_score'])df = pd.DataFrame(columns=['model', 'roc_auc', 'precision', 'f_score'])"
      ],
      "metadata": {
        "id": "1UmmpqdDo70L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_embedding(user_articles_list):\n",
        "    user_articles_list = eval(user_articles_list)\n",
        "    user_vector = np.array([doc_dict[doc_id] for doc_id in user_articles_list])\n",
        "    user_vector = np.mean(user_vector, 0)\n",
        "    return user_vector\n",
        "\n",
        "get_user_embedding(user_articles_list)\n",
        "\n",
        "user_embeddings = pd.DataFrame([i for i in users['articles'].apply(lambda x: get_user_embedding(x), 1)])\n",
        "user_embeddings.columns = ['topic_{}'.format(i) for i in range(25)]\n",
        "user_embeddings['uid'] = users['uid'].values\n",
        "user_embeddings = user_embeddings[['uid']+['topic_{}'.format(i) for i in range(25)]]\n",
        "\n",
        "X = pd.merge(user_embeddings, target, 'left')\n",
        "\n",
        "#разделим данные на train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X[['topic_{}'.format(i) for i in range(25)]],\n",
        "                                                X['churn'], random_state=0)\n",
        "\n",
        "logreg = LogisticRegression()\n",
        "#обучим\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "#наши прогнозы для тестовой выборки\n",
        "preds = logreg.predict_proba(X_test)[:, 1]\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, preds)\n",
        "fscore = (2 * precision * recall) / (precision + recall + 0.0000000001)\n",
        "# locate the index of the largest f score\n",
        "ix = np.argmax(fscore)\n",
        "print('Best Threshold=%f, F-Score=%.3f, Precision=%.3f, Recall=%.3f' % (thresholds[ix],\n",
        "                                                                        fscore[ix],\n",
        "                                                                        precision[ix],\n",
        "                                                                        recall[ix]))\n",
        "roc_auc = roc_auc_score(y_test, preds)\n",
        "\n",
        "df = df.append({'model': 'mean', 'roc_auc': roc_auc, 'precision': precision[ix], 'f_score': fscore[ix]}, ignore_index=True)"
      ],
      "metadata": {
        "id": "dOqAMt2ho_vV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_embedding(user_articles_list):\n",
        "    user_articles_list = eval(user_articles_list)\n",
        "    user_vector = np.array([doc_dict[doc_id] for doc_id in user_articles_list])\n",
        "    user_vector = np.median(user_vector, 0)\n",
        "    return user_vector\n",
        "\n",
        "get_user_embedding(user_articles_list)\n",
        "\n",
        "user_embeddings = pd.DataFrame([i for i in users['articles'].apply(lambda x: get_user_embedding(x), 1)])\n",
        "user_embeddings.columns = ['topic_{}'.format(i) for i in range(25)]\n",
        "user_embeddings['uid'] = users['uid'].values\n",
        "user_embeddings = user_embeddings[['uid']+['topic_{}'.format(i) for i in range(25)]]\n",
        "\n",
        "X = pd.merge(user_embeddings, target, 'left')\n",
        "\n",
        "#разделим данные на train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X[['topic_{}'.format(i) for i in range(25)]],\n",
        "                                                X['churn'], random_state=0)\n",
        "\n",
        "logreg = LogisticRegression()\n",
        "#обучим\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "#наши прогнозы для тестовой выборки\n",
        "preds = logreg.predict_proba(X_test)[:, 1]\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, preds)\n",
        "fscore = (2 * precision * recall) / (precision + recall + 0.0000000001)\n",
        "# locate the index of the largest f score\n",
        "ix = np.argmax(fscore)\n",
        "print('Best Threshold=%f, F-Score=%.3f, Precision=%.3f, Recall=%.3f' % (thresholds[ix],\n",
        "                                                                        fscore[ix],\n",
        "                                                                        precision[ix],\n",
        "                                                                        recall[ix]))\n",
        "roc_auc = roc_auc_score(y_test, preds)\n",
        "\n",
        "df = df.append({'model': 'median', 'roc_auc': roc_auc, 'precision': precision[ix], 'f_score': fscore[ix]}, ignore_index=True)"
      ],
      "metadata": {
        "id": "Ke17VAXppEK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_embedding(user_articles_list):\n",
        "    user_articles_list = eval(user_articles_list)\n",
        "    user_vector = np.array([doc_dict[doc_id] for doc_id in user_articles_list])\n",
        "    user_vector = np.max(user_vector, 0)\n",
        "    return user_vector\n",
        "\n",
        "get_user_embedding(user_articles_list)\n",
        "\n",
        "user_embeddings = pd.DataFrame([i for i in users['articles'].apply(lambda x: get_user_embedding(x), 1)])\n",
        "user_embeddings.columns = ['topic_{}'.format(i) for i in range(25)]\n",
        "user_embeddings['uid'] = users['uid'].values\n",
        "user_embeddings = user_embeddings[['uid']+['topic_{}'.format(i) for i in range(25)]]\n",
        "\n",
        "X = pd.merge(user_embeddings, target, 'left')\n",
        "\n",
        "#разделим данные на train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X[['topic_{}'.format(i) for i in range(25)]],\n",
        "                                                X['churn'], random_state=0)\n",
        "\n",
        "logreg = LogisticRegression()\n",
        "#обучим\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "#наши прогнозы для тестовой выборки\n",
        "preds = logreg.predict_proba(X_test)[:, 1]\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, preds)\n",
        "fscore = (2 * precision * recall) / (precision + recall + 0.0000000001)\n",
        "# locate the index of the largest f score\n",
        "ix = np.argmax(fscore)\n",
        "print('Best Threshold=%f, F-Score=%.3f, Precision=%.3f, Recall=%.3f' % (thresholds[ix],\n",
        "                                                                        fscore[ix],\n",
        "                                                                        precision[ix],\n",
        "                                                                        recall[ix]))\n",
        "roc_auc = roc_auc_score(y_test, preds)\n",
        "\n",
        "df = df.append({'model': 'max', 'roc_auc': roc_auc, 'precision': precision[ix], 'f_score': fscore[ix]}, ignore_index=True)"
      ],
      "metadata": {
        "id": "l1d18uaBpEax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "CnR1ieQEpLb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выводы: метод получения эмбеддингов пользователей MAX оказался эффективнее остальных т.к. при этом методе пики распределения не размываются, а напротив, подчеркиваются что в данном случае оказалось эффективнее.\n",
        "\n",
        "Предположение: возможно при другом объёме и содержании текста более эффективными окажутся \"сглаживающие\" методы."
      ],
      "metadata": {
        "id": "n_jzo0ggpOVs"
      }
    }
  ]
}