{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0l3SFrLWIlKYTXpDV+RGx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coldbilberry/repo-gui/blob/main/NLP_%D0%B2_%D0%97%D0%B4%D1%80%D0%B0%D0%B2%D0%BE%D0%BE%D1%85%D1%80%D0%B0%D0%BD%D0%B5%D0%BD%D0%B8%D0%B8_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SSeLOOj-gekD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "xhdGvZDrhYL7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk"
      ],
      "metadata": {
        "id": "3E2OCAyahd4n"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import time"
      ],
      "metadata": {
        "id": "hpPZ4Ww2hhJj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "u0c2QidThktS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "WBUyoQLChna_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "aeZbgVrQhuAp",
        "outputId": "492b2b3b-c8cc-45ff-f160-cf4557d785e9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-3b8a479202a4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "to_find_df = pd.read_csv('drive/MyDrive/Применение NLP в Здравоохранении/data/to_find.tsv', sep='|')\n",
        "concepts_df = pd.read_csv('drive/MyDrive/Применение NLP в Здравоохранении/data/концепты.tsv', sep='\\t')"
      ],
      "metadata": {
        "id": "-zbMFozMhz1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_find_df.head()"
      ],
      "metadata": {
        "id": "QDs7vtwIh0qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concepts_df.head()"
      ],
      "metadata": {
        "id": "OwIbyGiPh4RW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "1QC3yfY8h6g2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Инициализация инструментов для предобработки текста\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "uxOOo_RHh9D5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Токенизация текста\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    # Удаление стоп-слов и стемминг\n",
        "    tokens = [stemmer.stem(lemmatizer.lemmatize(token)) for token in tokens if token.isalnum() and token not in stop_words]\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "MsxivL79h_bC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Применение предобработки к данным\n",
        "to_find_df['processed_text'] = to_find_df['Text'].apply(preprocess_text)\n",
        "concepts_df['processed_concept'] = concepts_df['CONCEPT'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "PEK9mGEgiAAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_find_df.head()"
      ],
      "metadata": {
        "id": "GUqmEAj5iC89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concepts_df.head()"
      ],
      "metadata": {
        "id": "beIIa-c_iGz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Векторизация текста\n",
        "vectorizer = TfidfVectorizer()\n",
        "text_vectors = vectorizer.fit_transform(to_find_df['processed_text'])\n",
        "concept_vectors = vectorizer.transform(concepts_df['processed_concept'])"
      ],
      "metadata": {
        "id": "ds-Fqc9riHls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Вычисление косинусного сходства\n",
        "similarity_matrix = cosine_similarity(text_vectors, concept_vectors)"
      ],
      "metadata": {
        "id": "9oj60WadiKRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Количество топовых совпадений\n",
        "num_top_matches = 5"
      ],
      "metadata": {
        "id": "8JIhR6AsiMvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Поиск топовых совпадений\n",
        "tops = (-similarity_matrix).argsort()[:, :num_top_matches]"
      ],
      "metadata": {
        "id": "tNJspqKciPaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "result = pd.concat(\n",
        "    [\n",
        "        pd.concat(\n",
        "            [\n",
        "                to_find_df.loc[[i], ['Text', 'id']].reset_index(drop=True),\n",
        "                concepts_df.loc[[t], ['CUI', 'CONCEPT']].reset_index(drop=True),\n",
        "                pd.Series(similarity_matrix[i][t], name='Similarity')\n",
        "            ],\n",
        "            axis=1\n",
        "        )\n",
        "        for i in range(len(to_find_df)) for t in tops[i]\n",
        "    ]\n",
        ").reset_index(drop=True)\n",
        "\n",
        "elapsed_time = (time.time() - start_time) / 60\n",
        "\n",
        "result.head(), elapsed_time"
      ],
      "metadata": {
        "id": "9zeJ0HvJiRl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "sSF6HaaLiVFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Предобработка word2vec"
      ],
      "metadata": {
        "id": "wH_IZlnUiZ4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Предобработка текста с помощью word2vec\n",
        "def preprocess_for_word2vec(text):\n",
        "    return simple_preprocess(text, deacc=True)"
      ],
      "metadata": {
        "id": "flrVxVb8iXFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Применение предобработки\n",
        "to_find_df['processed_tokens'] = to_find_df['Text'].apply(preprocess_for_word2vec)\n",
        "concepts_df['processed_tokens'] = concepts_df['CONCEPT'].apply(preprocess_for_word2vec)"
      ],
      "metadata": {
        "id": "z9IToHPlidd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Объединение данных для обучения модели Word2Vec\n",
        "all_texts = to_find_df['processed_tokens'].tolist() + concepts_df['processed_tokens'].tolist()"
      ],
      "metadata": {
        "id": "ALSe0BA2ifmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Обучение модели Word2Vec\n",
        "word2vec_model = Word2Vec(sentences=all_texts, vector_size=100, window=5, min_count=1, workers=4)"
      ],
      "metadata": {
        "id": "zBFw0hupii7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция для получения среднего вектора текста\n",
        "def get_average_vector(words, model):\n",
        "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
        "    if vectors:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(model.vector_size)"
      ],
      "metadata": {
        "id": "eUMRB8sNijw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Преобразование текстов в векторные представления\n",
        "to_find_df['vector'] = to_find_df['processed_tokens'].apply(lambda tokens: get_average_vector(tokens, word2vec_model))\n",
        "concepts_df['vector'] = concepts_df['processed_tokens'].apply(lambda tokens: get_average_vector(tokens, word2vec_model))"
      ],
      "metadata": {
        "id": "cZo1H2S5imH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Вычисление косинусного сходства\n",
        "similarity_matrix = cosine_similarity(np.vstack(to_find_df['vector']), np.vstack(concepts_df['vector']))"
      ],
      "metadata": {
        "id": "NuvZNFyoiq0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Количество топовых совпадений\n",
        "num_top_matches = 5"
      ],
      "metadata": {
        "id": "74cE0DCjir5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Поиск топовых совпадений\n",
        "tops = (-similarity_matrix).argsort()[:, :num_top_matches]"
      ],
      "metadata": {
        "id": "gFWCRywIiuUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание таблицы с результатами\n",
        "result = pd.concat(\n",
        "    [\n",
        "        pd.concat(\n",
        "            [\n",
        "                to_find_df.loc[[i], ['Text', 'id']].reset_index(drop=True),\n",
        "                concepts_df.loc[[t], ['CUI', 'CONCEPT']].reset_index(drop=True),\n",
        "                pd.Series(similarity_matrix[i][t], name='Similarity')\n",
        "            ],\n",
        "            axis=1\n",
        "        )\n",
        "        for i in range(len(to_find_df)) for t in tops[i]\n",
        "    ]\n",
        ").reset_index(drop=True)"
      ],
      "metadata": {
        "id": "i8cXvChGiwPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "zVC9hbaAiyTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Предобработка BERT"
      ],
      "metadata": {
        "id": "seuchGEki2Hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "QMvlaI55i3Q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bert_embeddings(text, tokenizer, model):\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state[:, 0, :].numpy()"
      ],
      "metadata": {
        "id": "-Ns9fxV0i51G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Применение BERT для получения эмбеддингов текстов\n",
        "to_find_df['bert_embeddings'] = to_find_df['Text'].apply(lambda x: get_bert_embeddings(x, tokenizer, model))\n",
        "concepts_df['bert_embeddings'] = concepts_df['CONCEPT'].apply(lambda x: get_bert_embeddings(x, tokenizer, model))"
      ],
      "metadata": {
        "id": "iEpzm5gai9dV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Вычисление косинусного сходства между эмбеддингами\n",
        "similarity_matrix = cosine_similarity(\n",
        "    np.vstack(to_find_df['bert_embeddings']),\n",
        "    np.vstack(concepts_df['bert_embeddings'])\n",
        ")"
      ],
      "metadata": {
        "id": "F44xboN1i_zE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Количество топовых совпадений\n",
        "num_top_matches = 5"
      ],
      "metadata": {
        "id": "8ZAoxWTijDqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Поиск топовых совпадений\n",
        "tops = (-similarity_matrix).argsort()[:, :num_top_matches]"
      ],
      "metadata": {
        "id": "OC0xogBGjFxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание таблицы с результатами\n",
        "result = pd.concat(\n",
        "    [\n",
        "        pd.concat(\n",
        "            [\n",
        "                to_find_df.loc[[i], ['Text', 'id']].reset_index(drop=True),\n",
        "                concepts_df.loc[[t], ['CUI', 'CONCEPT']].reset_index(drop=True),\n",
        "                pd.Series(similarity_matrix[i][t], name='Similarity')\n",
        "            ],\n",
        "            axis=1\n",
        "        )\n",
        "        for i in range(len(to_find_df)) for t in tops[i]\n",
        "    ]\n",
        ").reset_index(drop=True)"
      ],
      "metadata": {
        "id": "hUK1IKpMjHzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "WTYf3RTdjKSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вывод:\n",
        "\n",
        "Использование BERT (Bidirectional Encoder Representations from Transformers) для оценки сходства между текстами существенно улучшает результаты Similarity благодаря его способности учитывать контекст и сложные семантические отношения."
      ],
      "metadata": {
        "id": "8we47IEnjP_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result.to_csv('result_BERT.csv', index=False, encoding='utf-8')"
      ],
      "metadata": {
        "id": "94OBUuWvjMvK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}