{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8yytLjOXN8+lxabSbbJls",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coldbilberry/repo-gui/blob/main/%D0%92%D0%B2%D0%B5%D0%B4%D0%B5%D0%BD%D0%B8%D0%B5_%D0%B2_%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D1%83_%D0%B5%D1%81%D1%82%D0%B5%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D0%BE%D0%B3%D0%BE_%D1%8F%D0%B7%D1%8B%D0%BA%D0%B0_%D0%A3%D1%80%D0%BE%D0%BA_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MWo0YVeIe47Q"
      },
      "outputs": [],
      "source": [
        "apostrophe_dict = {\n",
        "\"ain't\": \"am not / are not\",\n",
        "\"aren't\": \"are not / am not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he had / he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he shall / he will\",\n",
        "\"he'll've\": \"he shall have / he will have\",\n",
        "\"he's\": \"he has / he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how has / how is\",\n",
        "\"i'd\": \"I had / I would\",\n",
        "\"i'd've\": \"I would have\",\n",
        "\"i'll\": \"I shall / I will\",\n",
        "\"i'll've\": \"I shall have / I will have\",\n",
        "\"i'm\": \"I am\",\n",
        "\"i've\": \"I have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it had / it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it shall / it will\",\n",
        "\"it'll've\": \"it shall have / it will have\",\n",
        "\"it's\": \"it has / it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she had / she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she shall / she will\",\n",
        "\"she'll've\": \"she shall have / she will have\",\n",
        "\"she's\": \"she has / she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so as / so is\",\n",
        "\"that'd\": \"that would / that had\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that has / that is\",\n",
        "\"there'd\": \"there had / there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there has / there is\",\n",
        "\"they'd\": \"they had / they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they shall / they will\",\n",
        "\"they'll've\": \"they shall have / they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we had / we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what shall / what will\",\n",
        "\"what'll've\": \"what shall have / what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what has / what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when has / when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where has / where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who shall / who will\",\n",
        "\"who'll've\": \"who shall have / who will have\",\n",
        "\"who's\": \"who has / who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why has / why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you had / you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you shall / you will\",\n",
        "\"you'll've\": \"you shall have / you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\"\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "short_word_dict = {\n",
        "\"121\": \"one to one\",\n",
        "\"a/s/l\": \"age, sex, location\",\n",
        "\"adn\": \"any day now\",\n",
        "\"afaik\": \"as far as I know\",\n",
        "\"afk\": \"away from keyboard\",\n",
        "\"aight\": \"alright\",\n",
        "\"alol\": \"actually laughing out loud\",\n",
        "\"b4\": \"before\",\n",
        "\"b4n\": \"bye for now\",\n",
        "\"bak\": \"back at the keyboard\",\n",
        "\"bf\": \"boyfriend\",\n",
        "\"bff\": \"best friends forever\",\n",
        "\"bfn\": \"bye for now\",\n",
        "\"bg\": \"big grin\",\n",
        "\"bta\": \"but then again\",\n",
        "\"btw\": \"by the way\",\n",
        "\"cid\": \"crying in disgrace\",\n",
        "\"cnp\": \"continued in my next post\",\n",
        "\"cp\": \"chat post\",\n",
        "\"cu\": \"see you\",\n",
        "\"cul\": \"see you later\",\n",
        "\"cul8r\": \"see you later\",\n",
        "\"cya\": \"bye\",\n",
        "\"cyo\": \"see you online\",\n",
        "\"dbau\": \"doing business as usual\",\n",
        "\"fud\": \"fear, uncertainty, and doubt\",\n",
        "\"fwiw\": \"for what it's worth\",\n",
        "\"fyi\": \"for your information\",\n",
        "\"g\": \"grin\",\n",
        "\"g2g\": \"got to go\",\n",
        "\"ga\": \"go ahead\",\n",
        "\"gal\": \"get a life\",\n",
        "\"gf\": \"girlfriend\",\n",
        "\"gfn\": \"gone for now\",\n",
        "\"gmbo\": \"giggling my butt off\",\n",
        "\"gmta\": \"great minds think alike\",\n",
        "\"h8\": \"hate\",\n",
        "\"hagn\": \"have a good night\",\n",
        "\"hdop\": \"help delete online predators\",\n",
        "\"hhis\": \"hanging head in shame\",\n",
        "\"iac\": \"in any case\",\n",
        "\"ianal\": \"I am not a lawyer\",\n",
        "\"ic\": \"I see\",\n",
        "\"idk\": \"I don't know\",\n",
        "\"imao\": \"in my arrogant opinion\",\n",
        "\"imnsho\": \"in my not so humble opinion\",\n",
        "\"imo\": \"in my opinion\",\n",
        "\"iow\": \"in other words\",\n",
        "\"ipn\": \"I’m posting naked\",\n",
        "\"irl\": \"in real life\",\n",
        "\"jk\": \"just kidding\",\n",
        "\"l8r\": \"later\",\n",
        "\"ld\": \"later, dude\",\n",
        "\"ldr\": \"long distance relationship\",\n",
        "\"llta\": \"lots and lots of thunderous applause\",\n",
        "\"lmao\": \"laugh my ass off\",\n",
        "\"lmirl\": \"let's meet in real life\",\n",
        "\"lol\": \"laugh out loud\",\n",
        "\"ltr\": \"longterm relationship\",\n",
        "\"lulab\": \"love you like a brother\",\n",
        "\"lulas\": \"love you like a sister\",\n",
        "\"luv\": \"love\",\n",
        "\"m/f\": \"male or female\",\n",
        "\"m8\": \"mate\",\n",
        "\"milf\": \"mother I would like to fuck\",\n",
        "\"oll\": \"online love\",\n",
        "\"omg\": \"oh my god\",\n",
        "\"otoh\": \"on the other hand\",\n",
        "\"pir\": \"parent in room\",\n",
        "\"ppl\": \"people\",\n",
        "\"r\": \"are\",\n",
        "\"rofl\": \"roll on the floor laughing\",\n",
        "\"rpg\": \"role playing games\",\n",
        "\"ru\": \"are you\",\n",
        "\"shid\": \"slaps head in disgust\",\n",
        "\"somy\": \"sick of me yet\",\n",
        "\"sot\": \"short of time\",\n",
        "\"thanx\": \"thanks\",\n",
        "\"thx\": \"thanks\",\n",
        "\"ttyl\": \"talk to you later\",\n",
        "\"u\": \"you\",\n",
        "\"ur\": \"you are\",\n",
        "\"uw\": \"you’re welcome\",\n",
        "\"wb\": \"welcome back\",\n",
        "\"wfm\": \"works for me\",\n",
        "\"wibni\": \"wouldn't it be nice if\",\n",
        "\"wtf\": \"what the fuck\",\n",
        "\"wtg\": \"way to go\",\n",
        "\"wtgp\": \"want to go private\",\n",
        "\"ym\": \"young man\",\n",
        "\"gr8\": \"great\"\n",
        "}\n",
        "\n",
        "\n",
        "emoticon_dict = {\n",
        "\":)\": \"happy\",  # 1\n",
        "\":‑)\": \"happy\",  # 2\n",
        "\":-]\": \"happy\",\n",
        "\":-3\": \"happy\",\n",
        "\":->\": \"happy\",\n",
        "\"8-)\": \"happy\",  # 3\n",
        "\":-}\": \"happy\",\n",
        "\":o)\": \"happy\",  # 4\n",
        "\":c)\": \"happy\",  # 5\n",
        "\":^)\": \"happy\",  # 6\n",
        "\"=]\": \"happy\",\n",
        "\"=)\": \"happy\",  # 7\n",
        "\"<3\": \"happy\",\n",
        "\":-(\": \"sad\",  # 8\n",
        "\":(\": \"sad\",  # 9\n",
        "\":c\": \"sad\",\n",
        "\":<\": \"sad\",\n",
        "\":[\": \"sad\",  # 10\n",
        "\">:[\": \"sad\",  # 11\n",
        "\":{\": \"sad\",\n",
        "\">:(\": \"sad\",  # 12\n",
        "\":-c\": \"sad\",\n",
        "\":-<\": \"sad\",  # 13\n",
        "\":-[\": \"sad\",  # 14\n",
        "\":-||\": \"sad\"  # 15\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "import os\n",
        "\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "6dTa_umPf39P"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', None)"
      ],
      "metadata": {
        "id": "8Nvaw5c3lgjO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('./data/train_tweets.csv')\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "H2ZNdk_blsgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv('./data/test_tweets.csv')\n",
        "test_df.head()"
      ],
      "metadata": {
        "id": "QaqvAl70l1ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combine_df = train_df.append(test_df, ignore_index = True, sort = False)\n",
        "combine_df.head()"
      ],
      "metadata": {
        "id": "3eu6hauel4T2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(combine_df.info())"
      ],
      "metadata": {
        "id": "UuzHLcqfl61W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1)"
      ],
      "metadata": {
        "id": "sQ1F131EmxeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# help(re)"
      ],
      "metadata": {
        "id": "rBAGz0_sl_Jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(combine_df['tweet'][1])"
      ],
      "metadata": {
        "id": "DF_g_SShl_0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = re.findall(r'@[\\w]*', combine_df['tweet'][1])\n",
        "print(result)"
      ],
      "metadata": {
        "id": "AyA3DqJAmDI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# result = re.sub(r'@[\\w]* ', '', combine_df['tweet'][1])  # удаляем с пробелом после @user\n",
        "result = re.sub(r'@[\\w]*', '', combine_df['tweet'][1])  # пробелы остаются\n",
        "print(result)"
      ],
      "metadata": {
        "id": "8z92oktXmDMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    ''' Удалим @user из всех твитов с помощью паттерна \"@[\\w]*\" '''\n",
        "\n",
        "    return  re.sub(r'@[\\w]*', '', text)"
      ],
      "metadata": {
        "id": "nGUNF4wImKgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# N = 5\n",
        "# combine_df['tweet_1'] = combine_df['tweet'][:N].apply(clean_text)\n",
        "\n",
        "combine_df['tweet'] = combine_df['tweet'].apply(clean_text)"
      ],
      "metadata": {
        "id": "ut1EtpfDmNXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combine_df.head()"
      ],
      "metadata": {
        "id": "f5sxyHD0mPCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2)"
      ],
      "metadata": {
        "id": "aJZQ5qAPm0xl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def low_text(text):\n",
        "    ''' 2. Изменим регистр твитов на нижний с помощью .lower() '''\n",
        "\n",
        "    return  text.lower()"
      ],
      "metadata": {
        "id": "JVv4UEudm2FK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Удалим @user Из Всех Твитов С Помощью Паттерна \"@[\\w]*\" '\n",
        "print(text)\n",
        "print(low_text(text))"
      ],
      "metadata": {
        "id": "Vn6LdnBCm6cE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "combine_df['tweet'] = combine_df['tweet'].apply(clean_text)\n",
        "combine_df.head()"
      ],
      "metadata": {
        "id": "lTQEGriBm9TB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3)"
      ],
      "metadata": {
        "id": "LD6ZrxrKnBiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"can't\"\n",
        "apostrophe_dict[word]"
      ],
      "metadata": {
        "id": "Z6gt9Xw2nAVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replacement_text(text, dictionary):\n",
        "    \"\"\"\n",
        "    Для каждого слова в тексте проверить (for word in text.split()),\n",
        "    если слово есть в словаре apostrophe_dict в качестве ключа (сокращенного слова),\n",
        "    то заменить ключ на значение (полную версию слова).\n",
        "    \"\"\"\n",
        "    for word in text.split():\n",
        "        if word in dictionary:\n",
        "            text = re.sub(word, dictionary.get(word), text)\n",
        "\n",
        "    return  text"
      ],
      "metadata": {
        "id": "Yhonk-_3nFy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"i can't use cause they don't offer wheelchair vans in pdx\"\n",
        "print(text, '\\n' + replacement_text(text, apostrophe_dict))"
      ],
      "metadata": {
        "id": "SuJoM1jknLyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "combine_df['tweet'] = combine_df['tweet'].apply(replacement_text, dictionary = apostrophe_dict)\n",
        "combine_df.head(3)"
      ],
      "metadata": {
        "id": "PIxS32EfnOG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4)"
      ],
      "metadata": {
        "id": "xFJv5ITAnSLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"121 a/s/l lol ltr lulab lulas luv gr8\"\n",
        "# text = \"121, a/s/l\"\n",
        "# text = \"121 a/s/l adn afaik afk aight alol b4\"\n",
        "print(text, '\\n' + replacement_text(text, short_word_dict))"
      ],
      "metadata": {
        "id": "baCHbQYOnQh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "combine_df['tweet'] = combine_df['tweet'].apply(replacement_text, dictionary = short_word_dict)\n",
        "combine_df.head(3)"
      ],
      "metadata": {
        "id": "fTTjPfCmnX2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5)"
      ],
      "metadata": {
        "id": "E_HtpGMKnZSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replacement_emoticons(text, dictionary, flag=0):\n",
        "    \"\"\" Заменим эмотиконы (пример: \":)\" = \"happy\") на пробелы. \"\"\"\n",
        "\n",
        "#     print(text.split())\n",
        "#     print()\n",
        "    for emoticon in text.split():\n",
        "        word = emoticon\n",
        "        if emoticon in dictionary:\n",
        "            if emoticon == \":)\":  # 1\n",
        "                word = r\":\\)\"\n",
        "            elif emoticon == \":‑)\":  # 2\n",
        "                word = r\":‑\\)\"\n",
        "            elif emoticon == \"8-)\":  # 3\n",
        "                word = r\"8-\\)\"\n",
        "            elif emoticon == \":o)\":  # 4\n",
        "                word = r\":o\\)\"\n",
        "            elif emoticon == \":c)\":  # 5\n",
        "                word = r\":c\\)\"\n",
        "            elif  emoticon == \":^)\":  # 6\n",
        "                 word = r\":\\^\\)\"\n",
        "            elif  emoticon == \"=)\":  # 7\n",
        "                word = r\"=\\)\"\n",
        "            elif emoticon == \":-(\":  # 8\n",
        "                word = r\":-\\(\"\n",
        "            elif emoticon == \":(\":  # 9\n",
        "                word = r\":\\(\"\n",
        "            elif emoticon == \":[\":  # 10\n",
        "                word = r\":\\[\"\n",
        "\n",
        "            elif emoticon == \">:[\":  # 11\n",
        "                word = r\">:\\[\"\n",
        "            elif emoticon == \">:(\":  # 12\n",
        "                word = r\">:\\(\"\n",
        "\n",
        "            elif emoticon == \":-<\":  # 13\n",
        "                word = r\":-\\<\"\n",
        "            elif emoticon == \":-[\":  # 14\n",
        "                word = r\":-\\[\"\n",
        "            elif emoticon == \":-||\":  # 15\n",
        "                word = r\":-\\|\\|\"\n",
        "            if flag == 0 :\n",
        "                text = re.sub(word, dictionary.get(emoticon), text)  # замена на \"happy\" or \"sad\"\n",
        "            else:\n",
        "                text = re.sub(word, \" \", text)  # замена на пробел\n",
        "\n",
        "    return  text"
      ],
      "metadata": {
        "id": "2HQZPpv_naVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emoticon = \":) :‑) :-3 :-> 8-) :-} :o) :c) :( :c :< :[ >:[ >:( :-c :{ \"\n",
        "\n",
        "emoticon = \":) :‑) :-] :-3 :-> 8-) :-} :o) :c) :^) =] =) <3 :-( :( :c :< :[  >:[ :{ >:( :-c :-< :-[ :-||\"\n",
        "\n",
        "# emoticon = \" >:[ \\n >:( \"\n",
        "# emoticon = \":)\"  # 1\n",
        "# emoticon = \":‑)\"  # 2\n",
        "# emoticon = \"8-)\"  # 3\n",
        "# emoticon = \":o)\"  # 4\n",
        "# emoticon = \":c)\"  # 5\n",
        "# emoticon = \":^)\"  # 6\n",
        "# emoticon = \"=)\"  # 7\n",
        "# emoticon = \":-(\"  # 8\n",
        "# emoticon = \":(\"  # 9\n",
        "# emoticon = \":[\"  # 10\n",
        "# emoticon = \" >:[  >:[  >:[ \"  # 11\n",
        "# emoticon = \">:(\"  # 12\n",
        "# emoticon = \":-<\"  # 13\n",
        "# emoticon = \":-[\"  # 14\n",
        "# emoticon = \":-||\"  # 15\n",
        "# print('\"' + emoticon + '\": ', '\"' + emoticon_dict.get(emoticon) + '\"')\n",
        "\n",
        "print(emoticon.split())\n",
        "print()\n",
        "print(replacement_emoticons(emoticon, emoticon_dict, 0))\n",
        "print('*'*50)\n",
        "print(replacement_emoticons(emoticon, emoticon_dict, 1))"
      ],
      "metadata": {
        "id": "uiGqsD5vnfYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emoticon_dict.get(':)')"
      ],
      "metadata": {
        "id": "ch01WV0GnmD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "combine_df['tweet'] = combine_df['tweet'].apply(replacement_emoticons, dictionary = emoticon_dict, flag = 0)\n",
        "combine_df.head(3)"
      ],
      "metadata": {
        "id": "0Ul3kN3rnoqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6)"
      ],
      "metadata": {
        "id": "67RMxCDVnyPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'when a father is dysfunctional and is so selfish he drags his kids into his dysfunction. #run'\n",
        "opt = re.sub(r'[^\\w\\s]','', text)\n",
        "print(text)\n",
        "print(opt)"
      ],
      "metadata": {
        "id": "98hyDeSSnzfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replacement_punctuation(text):\n",
        "    \"\"\"\n",
        "    Заменим пунктуацию на пробелы.\n",
        "    \"\"\"\n",
        "    text = re.sub(r'[^\\w\\s]',' ', text)\n",
        "\n",
        "    return  text"
      ],
      "metadata": {
        "id": "6KDCPghrn5MJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "combine_df['tweet'] = combine_df['tweet'].apply(replacement_punctuation)\n",
        "combine_df.head(3)"
      ],
      "metadata": {
        "id": "9Obl_I1Ao0vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7)"
      ],
      "metadata": {
        "id": "98eAMnYoo20Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def replace_special_characters(text):\n",
        "#     \"\"\"\n",
        "#     Заменим пунктуацию на пробелы.\n",
        "#     \"\"\"\n",
        "#     text = re.sub(r'[^a-zA-Z0-9]',' ', text)\n",
        "\n",
        "#     return  text\n",
        "\n",
        "\n",
        "# combine_df['tweet_7'] = combine_df['tweet_6'].apply(replace_special_characters)"
      ],
      "metadata": {
        "id": "rynK4FCZo4Bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_patern(text, patern):\n",
        "    \"\"\"\n",
        "    Заменим патерн на пробелы.\n",
        "    \"\"\"\n",
        "    text = re.sub(patern, ' ', text)\n",
        "\n",
        "    return  text"
      ],
      "metadata": {
        "id": "k-cxX1Pgo7GQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "combine_df['tweet'] = combine_df['tweet'].apply(replace_patern, patern =  r'[^a-zA-Z0-9]')\n",
        "combine_df.head(3)"
      ],
      "metadata": {
        "id": "Q3qpV5o8o-0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8)"
      ],
      "metadata": {
        "id": "1qs5ZKSNpB-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'a1b2c3'\n",
        "patern = r'[^a-zA-Z]'\n",
        "replace_patern(text, patern)"
      ],
      "metadata": {
        "id": "Hq3NrXwKpA0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "combine_df['tweet'] = combine_df['tweet'].apply(replacement_text, dictionary = short_word_dict)\n",
        "# combine_df.head(10)"
      ],
      "metadata": {
        "id": "4tYJ1Ig5pFch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "combine_df['tweet'] = combine_df['tweet'].apply(replace_patern, patern = r'[^a-zA-Z]')\n",
        "combine_df.head()"
      ],
      "metadata": {
        "id": "4taqIssxpJhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9)"
      ],
      "metadata": {
        "id": "psymXFrJpLSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def slugify(x):\n",
        "    words = [w for w in x.split() if len(w)>1]\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "slugify(\"My test 1 2 3  string\")"
      ],
      "metadata": {
        "id": "vnRL1HRqpMXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "combine_df['tweet'] = combine_df['tweet'].apply(slugify)\n",
        "combine_df.head(3)"
      ],
      "metadata": {
        "id": "7rHNSOFXpPHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10)"
      ],
      "metadata": {
        "id": "GjLgMcoepR-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import tokenize as tknz"
      ],
      "metadata": {
        "id": "rv7PNUrPpUk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "combine_df['tweet_token'] = combine_df['tweet'].apply(tknz.word_tokenize)\n",
        "combine_df.head(3)"
      ],
      "metadata": {
        "id": "Efb7dWfapUo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(combine_df['tweet_token'][0])"
      ],
      "metadata": {
        "id": "VBrZO4bJpaFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11)"
      ],
      "metadata": {
        "id": "9GQ6u8Sjpbrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "text = \"This is a sentence in English that contains the SampleWord\"\n",
        "text_tokens = word_tokenize(text)\n",
        "print(type(text_tokens))\n",
        "print(text_tokens)\n",
        "remove_sw = [word for word in text_tokens if not word in nltk.corpus.stopwords.words()]\n",
        "\n",
        "print(remove_sw)"
      ],
      "metadata": {
        "id": "XZIQgKqgpaKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(token):\n",
        "    \"\"\"\n",
        "    Удалим стоп-слова из токенов.\n",
        "    \"\"\"\n",
        "\n",
        "    remove_sw = [word for word in token if not word in nltk.corpus.stopwords.words(\"english\")]\n",
        "\n",
        "    return  remove_sw"
      ],
      "metadata": {
        "id": "1dncPkFLpfd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(token, stop_words):\n",
        "    \"\"\"\n",
        "    Удалим стоп-слова из токенов.\n",
        "    \"\"\"\n",
        "\n",
        "    remove_sw = [word for word in token if not word in set(stop_words)]\n",
        "\n",
        "    return  remove_sw"
      ],
      "metadata": {
        "id": "78wrluB5phj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words_en = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "stop_words_en = nltk.corpus.stopwords.words(\"english\")\n",
        "\n",
        "type(stop_words_en)"
      ],
      "metadata": {
        "id": "uTU8s3A5pkzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# combine_df['tweet_token_filtered'] = combine_df['tweet_token'].apply(remove_stop_words)\n",
        "combine_df['tweet_token_filtered'] = combine_df['tweet_token'].apply(remove_stop_words, stop_words=stop_words_en)\n",
        "# combine_df['tweet_token_filtered'] = combine_df.apply(lambda row: [w for w in row['tweet_token'] if not w in stop_words], axis=1)\n",
        "combine_df.head(3)"
      ],
      "metadata": {
        "id": "_dH61_6apohj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token = combine_df['tweet_token'][0]"
      ],
      "metadata": {
        "id": "azY-DIELptvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combine_df.to_csv(\"./combine_df.csv\", index=False)  # Сохранение без индексации"
      ],
      "metadata": {
        "id": "9eeM9TK8pwH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine_df = pd.read_csv(\"./combine_df.csv\", index_col='id')\n",
        "# combine_df = pd.read_csv(\"./combine_df.csv\")\n",
        "# combine_df.head(3)"
      ],
      "metadata": {
        "id": "ZOwF_Eq_pwMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine_df.drop(columns = ['Unnamed: 0'], axis = 1, inplace=True)\n",
        "# combine_df.drop(combine_df.columns[[0]], axis = 1, inplace=True)"
      ],
      "metadata": {
        "id": "6zCnllwDpzfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12)"
      ],
      "metadata": {
        "id": "JgU5gHUTp2mf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer"
      ],
      "metadata": {
        "id": "ifslW0oup3nR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snowball = SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "mehNgEc0p64s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# words = word_tokenize(text_test)\n",
        "# %%time\n",
        "\n",
        "# combine_df['tweet_stemmed'] = combine_df['tweet_token_filtered'].apply(snowball.stem)\n",
        "combine_df['tweet_stemmed'] = combine_df.apply(lambda row: [snowball.stem(w) for w in row['tweet_token_filtered']], axis=1)\n",
        "combine_df.head(3)"
      ],
      "metadata": {
        "id": "mqzi1amxp97q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13)"
      ],
      "metadata": {
        "id": "AdMFWOUFqBoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "En8X6f-CqDFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "def get_lemmatizer(word, lemmatizer, pos):\n",
        "    \"\"\"\n",
        "    Print the results of stemmind and lemmitization using the passed stemmer, lemmatizer, word and pos (part of speech)\n",
        "    \"\"\"\n",
        "\n",
        "    return lemmatizer.lemmatize(word, pos)\n",
        "\n",
        "\n",
        "print(get_lemmatizer(word = \"seen\", lemmatizer = WordNetLemmatizer(), pos = wordnet.VERB))\n",
        "print(get_lemmatizer(word = \"drove\", lemmatizer = WordNetLemmatizer(), pos = wordnet.VERB))"
      ],
      "metadata": {
        "id": "TCEbUkeTqMcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lemmatizer(words, lemmatizer, pos):\n",
        "    \"\"\"\n",
        "    Print the results of stemmind and lemmitization using the passed stemmer, lemmatizer, word and pos (part of speech)\n",
        "    \"\"\"\n",
        "\n",
        "    lemmas = []\n",
        "    for word in words:\n",
        "        lemmas.append(lemmatizer.lemmatize(word, pos = nltk.corpus.wordnet.VERB) )\n",
        "\n",
        "    return lemmas\n",
        "\n",
        "print(get_lemmatizer(words = [\"seen\"], lemmatizer = WordNetLemmatizer(), pos = wordnet.VERB))\n",
        "print(get_lemmatizer(words = [\"drove\"], lemmatizer = WordNetLemmatizer(), pos = wordnet.VERB))"
      ],
      "metadata": {
        "id": "hpY1QyL7qPSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(combine_df['tweet_token_filtered'][1])\n",
        "print(get_lemmatizer(words = combine_df['tweet_token_filtered'][1], lemmatizer = WordNetLemmatizer(), pos = wordnet.VERB))"
      ],
      "metadata": {
        "id": "jUxikmKqqSKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "combine_df['tweet_lemmatized'] = \\\n",
        "        combine_df['tweet_token_filtered'].apply(get_lemmatizer, lemmatizer = WordNetLemmatizer(), pos = wordnet.VERB)\n",
        "\n",
        "combine_df.head(3)"
      ],
      "metadata": {
        "id": "xk0ePrnbqU_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combine_df.to_csv(\"./combine_df_total.csv\", index=False)  # Сохранение без индексации"
      ],
      "metadata": {
        "id": "K5CATWHxqYLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine_df = pd.read_csv(\"./combine_df_total.csv\")\n",
        "# combine_df.head(3)"
      ],
      "metadata": {
        "id": "ZaaDK0g0qaTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "combine_df['tweet_lemmatized'] = combine_df.apply(lambda row: [lemmatizer.lemmatize(w, pos = nltk.corpus.wordnet.VERB)\n",
        "                                                               for w in row['tweet_token_filtered']], axis=1)\n",
        "combine_df.head(5)"
      ],
      "metadata": {
        "id": "4QF_fWcHqaYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14)"
      ],
      "metadata": {
        "id": "649LSc0eqeuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combine_df.to_pickle(\"./dummy.pkl\")"
      ],
      "metadata": {
        "id": "i27DqEzxqiAI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}