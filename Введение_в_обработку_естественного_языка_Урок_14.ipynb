{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjqBdpvcIajEFKuGqIM7qr"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e5hTBusxsHxy"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "upload = files.upload()"
      ],
      "metadata": {
        "id": "em721XxNtgCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la"
      ],
      "metadata": {
        "id": "cuSQqqjvtlr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv 'dataset.jsonl' 'data/dataset.jsonl'"
      ],
      "metadata": {
        "id": "SxeaoFW9tvHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_PATH = 'data/dataset.jsonl'\n",
        "\n",
        "with open(DATASET_PATH) as f: # open the dataset file\n",
        "    df = pd.read_json(DATASET_PATH, lines=True).set_index('id') # read the .jsonl file into Pandas DataFrame"
      ],
      "metadata": {
        "id": "NxUYMN5Ntyid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.dtypes)\n",
        "# display(df)"
      ],
      "metadata": {
        "id": "W9RKRPEmt2CK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "JTZIWCpJt4Gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot rating histogram with outliers removed\n",
        "rating = df.rating.dropna()\n",
        "quantile = rating.quantile(.99)\n",
        "rating.hist(bins=100, range=(rating.min(), quantile))\n",
        "plt.xlabel('rating')\n",
        "plt.ylabel('count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sjBOAgm1t7Ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot quote count by year\n",
        "by_year = df.groupby(df.date.dt.year)['text'].count()\n",
        "by_year.plot.bar()\n",
        "plt.xlabel('year')\n",
        "plt.ylabel('count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vTcm5JEMt9Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot count of NaN and not-NaN rated quotes\n",
        "nans = df.rating.isna().sum()\n",
        "not_nans = len(df) - nans\n",
        "\n",
        "\n",
        "bars = [nans, not_nans]\n",
        "y_pos = np.arange(len(bars))\n",
        "plt.bar(y_pos, bars)\n",
        "plt.xticks(y_pos, ('NaN', 'Not NaN'))\n",
        "plt.ylabel('count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1CSxMlAct_h_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', 150)\n",
        "# pd.set_option('display.width', 500)"
      ],
      "metadata": {
        "id": "dQIOgVnsuBxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df['text'][0:5]\n",
        "df['text'][2]"
      ],
      "metadata": {
        "id": "Oijgx_8zuDZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 1"
      ],
      "metadata": {
        "id": "-mE24ygUulcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка твитов\n",
        "!wget https://gist.githubusercontent.com/avidale/d3da0ded85a4a16db6eb84d8331638ce/raw/a188084e5ef37b43b01fef0534b55c865b9a569e/tweets.txt"
      ],
      "metadata": {
        "id": "NIr3fujMuF0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "8Vkw0RR8uI83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Закачиваем сохранённые твиты\n",
        "with open('tweets.txt', 'r') as f:\n",
        "    tweets = f.read().strip().split('\\n\\n')\n",
        "print(len(tweets))\n",
        "for i in range(3):\n",
        "    print(tweets[i])"
      ],
      "metadata": {
        "id": "zAUtIOhRuKjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "zZQZzfI_uMiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "1BejVRAnuOQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Загружаем large GPT3, которая основана на GPT2\n",
        "model_name = 'sberbank-ai/rugpt3large_based_on_gpt2'\n",
        "#model_name = 'Grossmend/rudialogpt3_medium_based_on_gpt2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
      ],
      "metadata": {
        "id": "OzFYz0hcuQSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "15J_b3kVuVN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Мы хотим, чтобы GPT выводила суть, что после 3-х звёздочек ('***')\n",
        "#  нужно генерировать какой то осмысленный текст похожий на твит.\n",
        "#\n",
        "# Пишем рэндомное сэмплирование 5 твитов (берём 5 твитов из 26 случайным обазом)\n",
        "\n",
        "sep = '\\n***\\n'  # Признак того, что твит закончился и нужногенерировать ещё один твит\n",
        "# sep = '\\n27479153\tSandy_mustache\t2021-02-18 16:44:00\t'\n",
        "\n",
        "\n",
        "# Так как мы постоянно сэмплируем разные твиты,\n",
        "# мы будем постоянно получать разное распределение\n",
        "prefix = sep.join([''] + random.sample(tweets, k=5) + [''])\n",
        "\n",
        "tokens = tokenizer(prefix, return_tensors='pt')\n",
        "tokens = {k: v.to(model.device) for k, v in tokens.items()}\n",
        "end_token_id = tokenizer.encode('***')[0]  # '***' - токен который будет оканчивать твит\n",
        "\n",
        "# выводим то, что мы передаём на вход\n",
        "print(prefix)"
      ],
      "metadata": {
        "id": "7oTjeDpiuYFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Генерируем какой то осмысленный текст похожий на твит.\n",
        "\n",
        "size = tokens['input_ids'].shape[1]\n",
        "output = model.generate(\n",
        "    **tokens,\n",
        "    #end_token=end_token_id,\n",
        "    do_sample=False,  # вкл/выкл режим выдачи нескольких вариантов д.б. ещё один параметр\n",
        "    max_length=size+128,\n",
        "    # max_length=size+64,\n",
        "    repetition_penalty=4.2,  # штраф за повторы одинарных токенов\n",
        "    temperature=0.7,  # температура\n",
        "    num_beams=10,  # Строим дерево глубины 10\n",
        "    # no_repeat_ngram_size=3  # ! тройки подряд идущих токенов не должны повторяться (3 и меньше токенов не должны повторяться)\n",
        ")\n",
        "decoded = tokenizer.decode(output[0])\n",
        "result = decoded[len(prefix):]\n",
        "print(result)"
      ],
      "metadata": {
        "id": "T4QqIwZNuZIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Генерируем какой то осмысленный текст похожий на твит.\n",
        "\n",
        "size = tokens['input_ids'].shape[1]\n",
        "output = model.generate(\n",
        "    **tokens,\n",
        "    #end_token=end_token_id,\n",
        "    do_sample=False,\n",
        "    max_length=size+128,\n",
        "    # max_length=size+64,\n",
        "    repetition_penalty=4.2,   # штраф за повторы одинарных токенов\n",
        "    temperature=1.3,  # температура\n",
        "    num_beams=7,  # Строим дерево глубины 10\n",
        "    no_repeat_ngram_size=5  # ! тройки подряд идущих токенов не должны повторяться (3 и меньше токенов не должны повторяться)\n",
        ")\n",
        "decoded = tokenizer.decode(output[0])\n",
        "result = decoded[len(prefix):]\n",
        "print(result)"
      ],
      "metadata": {
        "id": "WExAjY40ub5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 2"
      ],
      "metadata": {
        "id": "kksFRqOQunKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install razdel networkx pymorphy2[fast] nltk rouge==0.3.1\n",
        "!pip install --upgrade datasets tqdm transformers"
      ],
      "metadata": {
        "id": "ppBV-s6busfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install datasets"
      ],
      "metadata": {
        "id": "GtS-qIv9uvtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "b6HQ--yvuxqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset_train = load_dataset('IlyaGusev/gazeta', revision=\"v1.0\", split= 'train[:10%]')\n",
        "dataset_test = load_dataset('IlyaGusev/gazeta', revision=\"v1.0\", split= 'test[:10%]')"
      ],
      "metadata": {
        "id": "VEls7450uz95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train"
      ],
      "metadata": {
        "id": "ylQTd9iAu1g2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test"
      ],
      "metadata": {
        "id": "fVuBIaH_u5Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test['summary'][0]"
      ],
      "metadata": {
        "id": "eFUX8-Jhu7Dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test['title'][0]"
      ],
      "metadata": {
        "id": "0OBqvesFu9ST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"IlyaGusev/rut5_base_sum_gazeta\""
      ],
      "metadata": {
        "id": "LGx-RyMcu_Q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def len_tok(text):\n",
        "    return len(text.split())"
      ],
      "metadata": {
        "id": "5--RBvbfvEZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len_sum, max_len_tl = max(map(len_tok, dataset_train['summary'])), max(map(len_tok, dataset_train['title']))\n",
        "max_len_sum, max_len_tl"
      ],
      "metadata": {
        "id": "dK5xgfzWvGdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len_sum, max_len_tl = 60, 15"
      ],
      "metadata": {
        "id": "63cb1noJvImK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def tokenize(batch):\n",
        "    tokenized_input = tokenizer(batch['summary'], padding='max_length', truncation=True, max_length=max_len_sum)\n",
        "    tokenized_label = tokenizer(batch['title'], padding='max_length', truncation=True, max_length=max_len_tl)\n",
        "\n",
        "    tokenized_input['labels'] = tokenized_label['input_ids']\n",
        "\n",
        "    return tokenized_input\n",
        "\n",
        "dataset_train = dataset_train.map(tokenize, batched=True, batch_size=8)\n",
        "dataset_test = dataset_test.map(tokenize, batched=True, batch_size=8)\n",
        "\n",
        "dataset_train.set_format('numpy', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "dataset_test.set_format('numpy', columns=['input_ids', 'attention_mask', 'labels'])"
      ],
      "metadata": {
        "id": "b6RCj7zbvLkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train.save_to_disk('gazeta/train')\n",
        "dataset_test.save_to_disk('gazeta/test')"
      ],
      "metadata": {
        "id": "1u2n9XPTvNPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "nuySEd2nvRCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "cA_4sCpwvTiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = 'gazeta/output'\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    eval_accumulation_steps=1, # Number of eval steps to keep in GPU (the higher, the mor vRAM used)\n",
        "    prediction_loss_only=True, # If I need co compute only loss and not other metrics, setting this to true will use less RAM\n",
        "    learning_rate=0.00001,\n",
        "    evaluation_strategy='steps', # Run evaluation every eval_steps\n",
        "    save_steps=1000, # How often to save a checkpoint\n",
        "    save_total_limit=1, # Number of maximum checkpoints to save\n",
        "    remove_unused_columns=True, # Removes useless columns from the dataset\n",
        "    run_name='run_gazeta', # Wandb run name\n",
        "    logging_steps=500, # How often to log loss to wandb\n",
        "    eval_steps=500, # How often to run evaluation on the val_set\n",
        "    logging_first_step=False, # Whether to log also the very first training step to wandb\n",
        "    load_best_model_at_end=True, # Whether to load the best model found at each evaluation.\n",
        "    metric_for_best_model=\"loss\", # Use loss to evaluate best model.\n",
        "    greater_is_better=False # Best model is the one with the lowest loss, not highest.\n",
        ")"
      ],
      "metadata": {
        "id": "HdblEmqwvWta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Обучение. У нас 10 эпох.\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset_train,\n",
        "    eval_dataset=dataset_test\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "GUhmw2ITvYON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(output_dir + '/model')"
      ],
      "metadata": {
        "id": "xfa7u1LCvdKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INX = 100\n",
        "print(\"SUMMARY: | {}\".format(dataset_test['summary'][INX]))\n",
        "print(\"TITLE: | {}\".format(dataset_test['title'][INX]))"
      ],
      "metadata": {
        "id": "Dyo3XZ5mvd7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\""
      ],
      "metadata": {
        "id": "6ggtJsOXvhrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "input_text = dataset_test['summary'][INX]\n",
        "\n",
        "with torch.no_grad():\n",
        "    tokenized_text = tokenizer(input_text, truncation=True, padding=True, return_tensors='pt')\n",
        "\n",
        "    source_ids = tokenized_text['input_ids'].to(device, dtype = torch.long)\n",
        "    source_mask = tokenized_text['attention_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "    generated_ids = model.generate(\n",
        "        input_ids = source_ids,\n",
        "        attention_mask = source_mask,\n",
        "        max_length=512,\n",
        "        num_beams=7,\n",
        "        temperature = 1.3,\n",
        "        repetition_penalty=1,\n",
        "        length_penalty=1,\n",
        "        early_stopping=True,\n",
        "        no_repeat_ngram_size=2  # количество повторов n-грамм > 2 запрещено.\n",
        "    )\n",
        "\n",
        "    # Параметры подбираются эксперементально\n",
        "\n",
        "    pred = tokenizer.decode(generated_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "\n",
        "print(\"\\noutput:\\n\" + pred)"
      ],
      "metadata": {
        "id": "vPKLomwUvkw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INX = 0\n",
        "print(\"SUMMARY: | {}\".format(dataset_test['summary'][INX]))\n",
        "print(\"TITLE: | {}\".format(dataset_test['title'][INX]))\n",
        "\n",
        "input_text = dataset_test['summary'][INX]\n",
        "\n",
        "with torch.no_grad():\n",
        "    tokenized_text = tokenizer(input_text, truncation=True, padding=True, return_tensors='pt')\n",
        "\n",
        "    source_ids = tokenized_text['input_ids'].to(device, dtype = torch.long)\n",
        "    source_mask = tokenized_text['attention_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "    generated_ids = model.generate(\n",
        "        input_ids = source_ids,\n",
        "        attention_mask = source_mask,\n",
        "        max_length=512,\n",
        "        num_beams=7,\n",
        "        temperature = 1.3,\n",
        "        repetition_penalty=1,\n",
        "        length_penalty=1,\n",
        "        early_stopping=True,\n",
        "        no_repeat_ngram_size=2  # количество повторов n-грамм > 2 запрещено.\n",
        "    )\n",
        "\n",
        "    # Параметры подбираются эксперементально\n",
        "    pred = tokenizer.decode(generated_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "\n",
        "print(\"\\noutput:\\n\" + pred)"
      ],
      "metadata": {
        "id": "Du9teMztvn61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вывод\n",
        "\n",
        "Модель сформировала достаточно приемлемый заголовок.\n",
        "\n",
        "Правда во втором случае он сообщает о событии как о случившемся, хотя оно ещё не произошло."
      ],
      "metadata": {
        "id": "AjHQmnr3vt86"
      }
    }
  ]
}