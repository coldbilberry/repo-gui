{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDpKn5G3zLSK/r7Z+qZ6Uf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coldbilberry/repo-gui/blob/main/%D0%A3%D1%80%D0%BE%D0%BA_10_%D0%A0%D0%B0%D1%81%D0%BF%D0%BE%D0%B7%D0%BD%D0%B0%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5_%D0%BB%D0%B8%D1%86_%D0%B8_%D1%8D%D0%BC%D0%BE%D1%86%D0%B8%D0%B9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRrVhiIbUwEF",
        "outputId": "aff8daea-29a8-493a-d3b6-38eaa0c3a9a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting facenet-pytorch==2.5.2\n",
            "  Downloading facenet_pytorch-2.5.2-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch==2.5.2) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch==2.5.2) (2.31.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch==2.5.2) (0.15.2+cu118)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch==2.5.2) (9.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch==2.5.2) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch==2.5.2) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch==2.5.2) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch==2.5.2) (2023.7.22)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchvision->facenet-pytorch==2.5.2) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->facenet-pytorch==2.5.2) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->facenet-pytorch==2.5.2) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->facenet-pytorch==2.5.2) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->facenet-pytorch==2.5.2) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->facenet-pytorch==2.5.2) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->facenet-pytorch==2.5.2) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision->facenet-pytorch==2.5.2) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision->facenet-pytorch==2.5.2) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchvision->facenet-pytorch==2.5.2) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchvision->facenet-pytorch==2.5.2) (1.3.0)\n",
            "Installing collected packages: facenet-pytorch\n",
            "Successfully installed facenet-pytorch-2.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install facenet-pytorch==2.5.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from glob import glob\n",
        "import sys, os\n",
        "\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from facenet_pytorch import MTCNN"
      ],
      "metadata": {
        "id": "t3h0rXH9ZFiU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "images = glob('/content/drive/MyDrive/leapGestRecog/leapGestRecog/**/**/*.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YW9JqoCZKq-",
        "outputId": "85881452-2a9b-41eb-89ca-efc7f8a3e7f5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [int(os.path.basename(img).split('_')[2])-1 for img in images] # отнимаем 1, чтобы были классы с 0\n",
        "\n",
        "images[:5], labels[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_omND1CBZnPF",
        "outputId": "59a3913e-4a95-4a85-d53d-eb4f498b03a7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([], [])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks7n5lpNZtPD",
        "outputId": "fe3ca58b-ebd6-4c52-f0ca-efcf31fc2943"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_nums_names = [(int((os.path.split(img)[0].split('/')[-1].split('_'))[0])-1, os.path.split(img)[0].split('/')[-1].split('_')[1:]) for img in images]\n",
        "nums_names_classes = {}\n",
        "for num, name in class_nums_names:\n",
        "    if num not in nums_names_classes.keys():\n",
        "        nums_names_classes[num] = name\n",
        "\n",
        "sorted(nums_names_classes.items(), key = lambda x: x[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mj4W__MWZ2jo",
        "outputId": "821b8e0b-2088-44e7-b9d5-83c8a0b1872d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def num_to_classname(num):\n",
        "    if num == 0:\n",
        "        return 'palm'\n",
        "    elif num == 1:\n",
        "        return 'l'\n",
        "    elif num == 2:\n",
        "        return 'fist'\n",
        "    elif num == 3:\n",
        "        return 'fist_moved'\n",
        "    elif num == 4:\n",
        "        return 'thumb'\n",
        "    elif num == 5:\n",
        "        return 'index'\n",
        "    elif num == 6:\n",
        "        return 'ok'\n",
        "    elif num == 7:\n",
        "        return 'palm_moved'\n",
        "    elif num == 8:\n",
        "        return 'c'\n",
        "    elif num == 9:\n",
        "        return 'down'"
      ],
      "metadata": {
        "id": "HrhGa7mpZ6Zm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train test split\n",
        "\n",
        "test_size = 0.3\n",
        "random_state = 1\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=test_size, random_state=random_state)"
      ],
      "metadata": {
        "id": "SfOykm3NZ71V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = Image.open(images[3])\n",
        "print(np.array(img).shape)\n",
        "img"
      ],
      "metadata": {
        "id": "yZBij2-paEVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform train & test data\n",
        "\n",
        "train_transformer = transforms.Compose([\n",
        "                         transforms.Grayscale(num_output_channels=1),\n",
        "                         transforms.Resize((48,48)),\n",
        "                         transforms.RandomHorizontalFlip(),\n",
        "                         transforms.RandomRotation(30),\n",
        "                         transforms.ToTensor()])\n",
        "\n",
        "val_transformer =  transforms.Compose([\n",
        "                         transforms.Grayscale(num_output_channels=1),\n",
        "                         transforms.Resize((48,48)),\n",
        "                         transforms.ToTensor()])"
      ],
      "metadata": {
        "id": "O4UtYWYxaJf0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GestureDataset(Dataset):\n",
        "    def __init__(self, images_gestures, labels, transformer):\n",
        "        self.images = images_gestures\n",
        "        self.labels = labels\n",
        "        self.transformer = transformer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img =  Image.open(self.images[idx])\n",
        "        img = self.transformer(img)\n",
        "\n",
        "        return img, self.labels[idx]"
      ],
      "metadata": {
        "id": "tRTlFudQaL1i"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = GestureDataset(X_train, y_train, train_transformer)\n",
        "val_dataset = GestureDataset(X_val, y_val, val_transformer)\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size, shuffle = True, drop_last = True)\n",
        "val_loader = DataLoader(val_dataset, batch_size, shuffle = False)"
      ],
      "metadata": {
        "id": "4eC1HU61aO6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, data in enumerate(train_loader):\n",
        "    print(f'Класс: {data[1][i]} - {num_to_classname(data[1][i])}')\n",
        "    print(data[0][i].shape)\n",
        "    plt.imshow(data[0][i].permute(1,2,0).squeeze())\n",
        "    plt.show()\n",
        "    if i>5:\n",
        "        break"
      ],
      "metadata": {
        "id": "yNcoyuF1aUBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_batch(train_loader):\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        fig, ax = plt.subplots(figsize=(12, 12))\n",
        "        ax.set_xticks([]); ax.set_yticks([])\n",
        "        print(f'image: {images[0].shape}')\n",
        "        ax.imshow(make_grid(images[:16], nrow=4).permute(1, 2, 0))\n",
        "        break\n",
        "\n",
        "show_batch(train_loader)"
      ],
      "metadata": {
        "id": "ZAabdwVgaXMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0dCTMa0DaY6e",
        "outputId": "d1138300-2c8a-4dab-917f-4b61e18c018e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Class ResNet\n",
        "\n",
        "def conv_block(in_channels, out_channels, pool=False):\n",
        "\n",
        "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "              nn.BatchNorm2d(out_channels),\n",
        "              nn.ELU(inplace=True)]\n",
        "    if pool:\n",
        "        layers.append(nn.MaxPool2d(2))\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "def linear_block(input_dim, output_dim, activation = False, dropout = 0.3):\n",
        "\n",
        "    layers = [nn.Linear(input_dim, output_dim),\n",
        "             nn.Dropout(dropout)]\n",
        "    if activation:\n",
        "        layers.append(nn.ReLU())\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = conv_block(in_channels, 128)\n",
        "        self.conv2 = conv_block(128, 128, pool=True)\n",
        "        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n",
        "        self.drop1 = nn.Dropout(0.5)\n",
        "\n",
        "        self.conv3 = conv_block(128, 256)\n",
        "        self.conv4 = conv_block(256, 256, pool=True)\n",
        "        self.res2 = nn.Sequential(conv_block(256, 256), conv_block(256, 256))\n",
        "        self.drop2 = nn.Dropout(0.5)\n",
        "\n",
        "        self.conv5 = conv_block(256, 512)\n",
        "        self.conv6 = conv_block(512, 512, pool=True)\n",
        "        self.res3 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n",
        "        self.drop3 = nn.Dropout(0.5)\n",
        "\n",
        "        self.classifier = nn.Sequential(nn.MaxPool2d(6),\n",
        "                                        nn.Flatten(),\n",
        "                                        nn.Linear(512, num_classes))\n",
        "\n",
        "    def forward(self, xb):\n",
        "        out = self.conv1(xb)\n",
        "        out = self.conv2(out)\n",
        "        out = self.res1(out) + out\n",
        "        out = self.drop1(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.res2(out) + out\n",
        "        out = self.drop2(out)\n",
        "\n",
        "        out = self.conv5(out)\n",
        "        out = self.conv6(out)\n",
        "        out = self.res3(out) + out\n",
        "        out = self.drop3(out)\n",
        "\n",
        "        out = self.classifier(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "B56qJ0zaabnL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize & compile the model\n",
        "\n",
        "net = ResNet(1, len(nums_names_classes)).to(device)\n",
        "\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.Adam(net.parameters(), learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTpyy9xWaf5-",
        "outputId": "75bb4f43-0879-41d2-de3d-8f55fbd54eba"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/init.py:405: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (img, label) in enumerate(train_loader):\n",
        "    output = net(img[i][None].to(device))\n",
        "    print(f'Real gesture: {int(label[i]), num_to_classname(int(label[i]))}, '\n",
        "          f'Predicted gesture: {int(output.argmax(1)), num_to_classname(int(output.argmax(1)))}')\n",
        "    plt.imshow(img[i].permute(1,2,0).squeeze())\n",
        "    break"
      ],
      "metadata": {
        "id": "D585U7uvaj3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model learning\n",
        "\n",
        "epochs = 15\n",
        "\n",
        "epoch_losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    train_running_loss = 0.0\n",
        "    total_acc_train = 0.0\n",
        "    epoch_loss = []\n",
        "    for data, labels in tqdm(train_loader):\n",
        "        data = data.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = net(data)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        train_running_loss += loss.item()\n",
        "        epoch_loss.append(loss.item())\n",
        "\n",
        "        acc = (outputs.argmax(dim=1) == labels).sum().item()\n",
        "        total_acc_train += acc\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "    val_running_loss, total_acc_val = 0.0, 0.0\n",
        "    val_epoch_loss = []\n",
        "    for data, labels in val_loader:\n",
        "        net.eval()\n",
        "        data = data.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = net(data)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        val_running_loss += loss.item()\n",
        "        val_epoch_loss.append(loss.item())\n",
        "\n",
        "        acc = (outputs.argmax(dim=1) == labels).sum().item()\n",
        "        total_acc_val += acc\n",
        "\n",
        "    print(f'Epoch {epoch+1}, loss:, {np.mean(epoch_loss)}, Train acc:, {total_acc_train / len(train_dataset):.3f}  '\n",
        "          f'Val loss:, {np.mean(val_epoch_loss)}, Val acc: {total_acc_val / len(val_dataset):.3f}')\n",
        "    epoch_losses.append(epoch_loss)"
      ],
      "metadata": {
        "id": "lLVH7LYdalm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot loss\n",
        "\n",
        "losses = [np.mean(loss) for loss in epoch_losses]\n",
        "plt.plot(losses, '-x')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('losses');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "zzyF7meoaqkx",
        "outputId": "22b4ebb6-1c06-4c55-c53a-76d40b93d422"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "\n",
        "model_path = './gesture_classification_model.pth'\n",
        "torch.save(net, model_path)"
      ],
      "metadata": {
        "id": "uWzMwQhcassO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "\n",
        "net = torch.load(model_path)\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(val_loader, 3):\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        outputs = net(images)\n",
        "        plt.title(f'pred - {num_to_classname(outputs[0].argmax())}, gt - {num_to_classname(labels[0])}')\n",
        "        plt.imshow(images[0].cpu().squeeze(), cmap='gray')\n",
        "        plt.show()\n",
        "        if i>10:\n",
        "            break"
      ],
      "metadata": {
        "id": "-XV7Sb3nawOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "from facenet_pytorch import MTCNN"
      ],
      "metadata": {
        "id": "EH0X7UyvazbR"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_block(in_channels, out_channels, pool=False):\n",
        "\n",
        "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "              nn.BatchNorm2d(out_channels),\n",
        "              nn.ELU(inplace=True)]\n",
        "    if pool:\n",
        "        layers.append(nn.MaxPool2d(2))\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "# Блок линейных слоев\n",
        "def linear_block(input_dim, output_dim, activation = False, dropout = 0.3):\n",
        "\n",
        "    layers = [nn.Linear(input_dim, output_dim),\n",
        "             nn.Dropout(dropout)]\n",
        "    if activation:\n",
        "        layers.append(nn.ReLU())\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "#  Сеть\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        # Конволюционная часть сети\n",
        "        self.conv1 = conv_block(in_channels, 128)\n",
        "        self.conv2 = conv_block(128, 128, pool=True)\n",
        "        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n",
        "        self.drop1 = nn.Dropout(0.5)\n",
        "\n",
        "        self.conv3 = conv_block(128, 256)\n",
        "        self.conv4 = conv_block(256, 256, pool=True)\n",
        "        self.res2 = nn.Sequential(conv_block(256, 256), conv_block(256, 256))\n",
        "        self.drop2 = nn.Dropout(0.5)\n",
        "\n",
        "        self.conv5 = conv_block(256, 512)\n",
        "        self.conv6 = conv_block(512, 512, pool=True)\n",
        "        self.res3 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n",
        "        self.drop3 = nn.Dropout(0.5)\n",
        "\n",
        "\n",
        "        self.max_pool = nn.MaxPool2d(6)\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.ff1 = linear_block(512, 256, activation = True)\n",
        "        self.ff2 = linear_block(256, 128, activation = True)\n",
        "        self.classifier = linear_block(128, num_classes)\n",
        "\n",
        "    def forward(self, xb):\n",
        "        #Convolution layers\n",
        "        out = self.conv1(xb)\n",
        "        out = self.conv2(out)\n",
        "        out = self.res1(out)\n",
        "        out = self.drop1(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.res2(out) + out\n",
        "        out = self.drop2(out)\n",
        "\n",
        "        out = self.conv5(out)\n",
        "        out = self.conv6(out)\n",
        "        out = self.res3(out) + out\n",
        "        out = self.drop3(out)\n",
        "\n",
        "        out = self.max_pool(out)\n",
        "        out = self.flatten(out)\n",
        "\n",
        "        # Feed Forward layers\n",
        "        out = self.ff1(out)\n",
        "        out = self.ff2(out)\n",
        "        out = self.classifier(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "3hGWNDNea0vH"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gesture_clf = ResNet(1, 10)\n",
        "gesture_clf = torch.load(model_path)"
      ],
      "metadata": {
        "id": "mcVzOs8Xa4Xb"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class FaceDetector\n",
        "\n",
        "class FaceDetector(object):\n",
        "    \"\"\"\n",
        "    Face detector class\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mtcnn):\n",
        "        self.mtcnn = mtcnn\n",
        "        self.gestmodel = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "        self.gestmodel.eval()\n",
        "\n",
        "    def _draw(self, frame, boxes, landmarks, gesture):\n",
        "        \"\"\"\n",
        "        Draw landmarks and boxes for each face detected\n",
        "        \"\"\"\n",
        "        try:\n",
        "            for box, ld in zip(boxes, landmarks):\n",
        "                cv2.rectangle(frame, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])),\n",
        "                              (255,255,0), thickness=2)\n",
        "\n",
        "                cv2.putText(frame,\n",
        "                    gesture, (int(box[2]), int(box[3])), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
        "\n",
        "                cv2.circle(frame, tuple([int(i) for i in ld[0]]), 5, (255,255,0), -1)\n",
        "                cv2.circle(frame, tuple([int(i) for i in ld[1]]), 5, (255,255,0), -1)\n",
        "                cv2.circle(frame, tuple([int(i) for i in ld[2]]), 5, (255,255,0), -1)\n",
        "                cv2.circle(frame, tuple([int(i) for i in ld[3]]), 5, (255,255,0), -1)\n",
        "                cv2.circle(frame, tuple([int(i) for i in ld[4]]), 5, (255,255,0), -1)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return frame\n",
        "\n",
        "    @staticmethod\n",
        "    def crop_faces(frame, boxes):\n",
        "        faces = []\n",
        "        for i, box in enumerate(boxes):\n",
        "            faces.append(frame[int(box[1]):int(box[3]),\n",
        "                int(box[0]):int(box[2])])\n",
        "        return faces\n",
        "\n",
        "    @staticmethod\n",
        "    def transform_frame(frame):\n",
        "        transformer = transforms.Compose([\n",
        "                      transforms.ToPILImage(),\n",
        "                      transforms.Grayscale(num_output_channels=1),\n",
        "                      transforms.Resize((48,48)),\n",
        "                      transforms.ToTensor()])\n",
        "\n",
        "        return transformer(frame).unsqueeze(1)\n",
        "\n",
        "    @staticmethod\n",
        "    def num_to_classname(num):\n",
        "        if num == 0:\n",
        "            return 'palm'\n",
        "        elif num == 1:\n",
        "            return 'l'\n",
        "        elif num == 2:\n",
        "            return 'fist'\n",
        "        elif num == 3:\n",
        "            return 'fist_moved'\n",
        "        elif num == 4:\n",
        "            return 'thumb'\n",
        "        elif num == 5:\n",
        "            return 'index'\n",
        "        elif num == 6:\n",
        "            return 'ok'\n",
        "        elif num == 7:\n",
        "            return 'palm_moved'\n",
        "        elif num == 8:\n",
        "            return 'c'\n",
        "        elif num == 9:\n",
        "            return 'down'\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def reactions_to_gestures(gesture):\n",
        "        if gesture == 'thumb':\n",
        "            plt.imshow(cv2.cvtColor(cv2.imread('./imgs reactions/good jıb.jpeg'), cv2.COLOR_BGR2RGB))\n",
        "            plt.show()\n",
        "            print(f\"Hi! See your photo below, you're amazing today :D. Your gesture is {gesture}, am I right?\")\n",
        "\n",
        "        elif gesture == 'ok':\n",
        "            plt.imshow(cv2.cvtColor(cv2.imread('./imgs reactions/ok.jpeg'), cv2.COLOR_BGR2RGB))\n",
        "            plt.show()\n",
        "            print(f\"Hey, you're showing OK! See your photo below! :D. I think, your gesture is {gesture}, am I right?\")\n",
        "\n",
        "        elif gesture == 'palm' or gesture == 'palm_moved':\n",
        "            plt.imshow(cv2.cvtColor(cv2.imread('./imgs reactions/palm.jpeg'), cv2.COLOR_BGR2RGB))\n",
        "            plt.show()\n",
        "            print(f\"Hi! See your photo below, you're amazing today :D. Your gesture is {gesture}, am I right?\")\n",
        "\n",
        "        elif gesture == 'fist' or  gesture == 'fist_moved':\n",
        "            print(f\"Hey, why you angry today? :( I think, your gesture is {gesture}, am I right?\")\n",
        "            plt.imshow(cv2.cvtColor(cv2.imread('./imgs reactions/fist.jpeg'), cv2.COLOR_BGR2RGB))\n",
        "            plt.show()\n",
        "\n",
        "        else:\n",
        "            game(15)\n",
        "            print(f'Hi! I think, your gesture is {gesture}, am I right?')\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def game(turns):\n",
        "        print(f\"Let's play the game! Try to guess the word letter by letter!You have {turns} attempts!\")\n",
        "\n",
        "        want_to_play= input('Do you want to play? Enter Y or N: ')[0]\n",
        "        want_to_play.lower()\n",
        "\n",
        "        if want_to_play == 'y':\n",
        "\n",
        "            wordList = [\"python\", \"pytorch\", \"nets\", \"framework\"]\n",
        "            shuffle(wordList)\n",
        "            word = wordList.pop()\n",
        "\n",
        "            guesses = \"\"\n",
        "\n",
        "            while turns > 0:\n",
        "                wrong = 0\n",
        "\n",
        "                for letter in word:\n",
        "                    if letter in guesses:\n",
        "                        print(letter, end=\" \")\n",
        "                    else:\n",
        "                        print(\"_\", end=\" \")\n",
        "                        wrong += 1\n",
        "\n",
        "                print(\"\\n\")\n",
        "\n",
        "                if wrong == 0:\n",
        "                    print(\"You win!!! :)\")\n",
        "                    break\n",
        "\n",
        "                guess = \"\"\n",
        "                guess = input(\"Enter the ENGLISH letter and press 'enter': \")[0]\n",
        "                guess.lower()\n",
        "                if guess in guesses:\n",
        "                    print(\"You had entered this letter before\")\n",
        "                guesses += guess\n",
        "\n",
        "                turns -=1\n",
        "                if guess not in word:\n",
        "                    print(f\"No letter {guess} in this word \")\n",
        "\n",
        "                else:\n",
        "                    print(f\"Correct! Letter {guess} contains in our word \")\n",
        "\n",
        "\n",
        "                print(f\"You have {turns} attempts left\")\n",
        "\n",
        "\n",
        "                if turns == 0:\n",
        "                    print(\"Sorry, you lose :(\")\n",
        "\n",
        "        if want_to_play == 'n':\n",
        "            print('See you next time! Bye;)')\n",
        "\n",
        "\n",
        "    def run(self):\n",
        "\n",
        "        try:\n",
        "            cap = cv2.VideoCapture(0)\n",
        "            for i in range(3):\n",
        "                cap.read()\n",
        "            ret, frame = cap.read()\n",
        "\n",
        "            boxes, probs, landmarks = self.mtcnn.detect(frame, landmarks=True)\n",
        "\n",
        "            face = self.crop_faces(frame, boxes)[0]\n",
        "\n",
        "            frame_for_model = self.transform_frame(frame)\n",
        "            gesture = self.gestmodel(frame_for_model)\n",
        "            gesture = self.num_to_classname(gesture.argmax())\n",
        "            self.reactions_to_gestures(gesture)\n",
        "\n",
        "            self._draw(frame, boxes, landmarks, gesture)\n",
        "\n",
        "            plt.imshow(frame)\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        cap.release()\n",
        "        cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "HV-FaTU2a-Ze"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FIRST run\n",
        "mtcnn = MTCNN()\n",
        "fcd = FaceDetector(mtcnn)\n",
        "fcd.run()"
      ],
      "metadata": {
        "id": "hgJ7qTQLbBBi"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}