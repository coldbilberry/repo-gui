{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOir0zKj00NwiE8ICA3CHkC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coldbilberry/repo-gui/blob/main/%D0%A3%D1%80%D0%BE%D0%BA_5_%D0%A0%D0%B5%D0%BA%D1%83%D1%80%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B5_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5_%D1%81%D0%B5%D1%82%D0%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dUC-kCbBo7Q4"
      },
      "outputs": [],
      "source": [
        "# importing the tensorflow package\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.test.is_built_with_cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FX6HA6Z7pTyK",
        "outputId": "272a70a7-9195-4db6-a129-164c6aa68254"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8X7oIbffpYbW",
        "outputId": "486c2c31-fb37-42da-923a-e34f0073c33d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 1."
      ],
      "metadata": {
        "id": "P7D8BTCapigt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.datasets import imdb"
      ],
      "metadata": {
        "id": "8XwwO5vdplAW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 100000\n",
        "\n",
        "# обрезание текстов после данного количества слов (среди top max_features наиболее используемые слова)\n",
        "maxlen = 150\n",
        "batch_size = 300 # увеличьте значение для ускорения обучения"
      ],
      "metadata": {
        "id": "Zqh0ZqdVpptn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Загрузка данных...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-lJlFu-ps--",
        "outputId": "4ac12ee6-7271-400c-ab74-266b7527d496"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Загрузка данных...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGROSqzqpxRH",
        "outputId": "b0810a32-8e62-46ab-c7d1-4b3a448d8292"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 14,\n",
              " 22,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 973,\n",
              " 1622,\n",
              " 1385,\n",
              " 65,\n",
              " 458,\n",
              " 4468,\n",
              " 66,\n",
              " 3941,\n",
              " 4,\n",
              " 173,\n",
              " 36,\n",
              " 256,\n",
              " 5,\n",
              " 25,\n",
              " 100,\n",
              " 43,\n",
              " 838,\n",
              " 112,\n",
              " 50,\n",
              " 670,\n",
              " 22665,\n",
              " 9,\n",
              " 35,\n",
              " 480,\n",
              " 284,\n",
              " 5,\n",
              " 150,\n",
              " 4,\n",
              " 172,\n",
              " 112,\n",
              " 167,\n",
              " 21631,\n",
              " 336,\n",
              " 385,\n",
              " 39,\n",
              " 4,\n",
              " 172,\n",
              " 4536,\n",
              " 1111,\n",
              " 17,\n",
              " 546,\n",
              " 38,\n",
              " 13,\n",
              " 447,\n",
              " 4,\n",
              " 192,\n",
              " 50,\n",
              " 16,\n",
              " 6,\n",
              " 147,\n",
              " 2025,\n",
              " 19,\n",
              " 14,\n",
              " 22,\n",
              " 4,\n",
              " 1920,\n",
              " 4613,\n",
              " 469,\n",
              " 4,\n",
              " 22,\n",
              " 71,\n",
              " 87,\n",
              " 12,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 38,\n",
              " 76,\n",
              " 15,\n",
              " 13,\n",
              " 1247,\n",
              " 4,\n",
              " 22,\n",
              " 17,\n",
              " 515,\n",
              " 17,\n",
              " 12,\n",
              " 16,\n",
              " 626,\n",
              " 18,\n",
              " 19193,\n",
              " 5,\n",
              " 62,\n",
              " 386,\n",
              " 12,\n",
              " 8,\n",
              " 316,\n",
              " 8,\n",
              " 106,\n",
              " 5,\n",
              " 4,\n",
              " 2223,\n",
              " 5244,\n",
              " 16,\n",
              " 480,\n",
              " 66,\n",
              " 3785,\n",
              " 33,\n",
              " 4,\n",
              " 130,\n",
              " 12,\n",
              " 16,\n",
              " 38,\n",
              " 619,\n",
              " 5,\n",
              " 25,\n",
              " 124,\n",
              " 51,\n",
              " 36,\n",
              " 135,\n",
              " 48,\n",
              " 25,\n",
              " 1415,\n",
              " 33,\n",
              " 6,\n",
              " 22,\n",
              " 12,\n",
              " 215,\n",
              " 28,\n",
              " 77,\n",
              " 52,\n",
              " 5,\n",
              " 14,\n",
              " 407,\n",
              " 16,\n",
              " 82,\n",
              " 10311,\n",
              " 8,\n",
              " 4,\n",
              " 107,\n",
              " 117,\n",
              " 5952,\n",
              " 15,\n",
              " 256,\n",
              " 4,\n",
              " 31050,\n",
              " 7,\n",
              " 3766,\n",
              " 5,\n",
              " 723,\n",
              " 36,\n",
              " 71,\n",
              " 43,\n",
              " 530,\n",
              " 476,\n",
              " 26,\n",
              " 400,\n",
              " 317,\n",
              " 46,\n",
              " 7,\n",
              " 4,\n",
              " 12118,\n",
              " 1029,\n",
              " 13,\n",
              " 104,\n",
              " 88,\n",
              " 4,\n",
              " 381,\n",
              " 15,\n",
              " 297,\n",
              " 98,\n",
              " 32,\n",
              " 2071,\n",
              " 56,\n",
              " 26,\n",
              " 141,\n",
              " 6,\n",
              " 194,\n",
              " 7486,\n",
              " 18,\n",
              " 4,\n",
              " 226,\n",
              " 22,\n",
              " 21,\n",
              " 134,\n",
              " 476,\n",
              " 26,\n",
              " 480,\n",
              " 5,\n",
              " 144,\n",
              " 30,\n",
              " 5535,\n",
              " 18,\n",
              " 51,\n",
              " 36,\n",
              " 28,\n",
              " 224,\n",
              " 92,\n",
              " 25,\n",
              " 104,\n",
              " 4,\n",
              " 226,\n",
              " 65,\n",
              " 16,\n",
              " 38,\n",
              " 1334,\n",
              " 88,\n",
              " 12,\n",
              " 16,\n",
              " 283,\n",
              " 5,\n",
              " 16,\n",
              " 4472,\n",
              " 113,\n",
              " 103,\n",
              " 32,\n",
              " 15,\n",
              " 16,\n",
              " 5345,\n",
              " 19,\n",
              " 178,\n",
              " 32]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the word index file mapping words to indices\n",
        "word_index = imdb.get_word_index()\n",
        "# Reverse the word index to obtain a dict mapping indices to words\n",
        "inverted_word_index = dict((i, word) for (word, i) in word_index.items())\n",
        "# Decode the first sequence in the dataset\n",
        "decoded_sequence = \" \".join(inverted_word_index[i] for i in x_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrwTfz7Ip3se",
        "outputId": "f46afbb7-e03e-4f1b-ee2b-3e64fcd5ff5e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1641221/1641221 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_sequence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "opUXe_bPp7Y7",
        "outputId": "cd916a73-e553-479b-9ae1-8d1406f121d1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"the as you with out themselves powerful lets loves their becomes reaching had journalist of lot from anyone to have after out atmosphere never more room titillate it so heart shows to years of every never going villaronga help moments or of every chest visual movie except her was several of enough more with is now current film as you of mine potentially unfortunately of you than him that with out themselves her get for was camp of you movie sometimes movie that with scary but pratfalls to story wonderful that in seeing in character to of 70s musicians with heart had shadows they of here that with her serious to have does when from why what have critics they is you that isn't one will very to as itself with other tricky in of seen over landed for anyone of gilmore's br show's to whether from than out themselves history he name half some br of 'n odd was two most of mean for 1 any an boat she he should is thought frog but of script you not while history he heart to real at barrel but when from one bit then have two of script their with her nobody most that with wasn't to with armed acting watch an for with heartfelt film want an\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x_train), 'тренировочные последовательности')\n",
        "print(len(x_test), 'тестовые последовательности')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6LD2nadp9ks",
        "outputId": "48e21cc8-5310-4001-9178-8edd5be6dc7d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25000 тренировочные последовательности\n",
            "25000 тестовые последовательности\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(x_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YrlHO-VqLE2",
        "outputId": "79d7c937-526e-4dee-e971-e536be4381d7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "218"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYzcn75dqO-3",
        "outputId": "b9ce379e-e0ce-44fb-c1ca-7f10f7aa3aa3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (25000, 150)\n",
            "x_test shape: (25000, 150)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8saXqhaYqUwN",
        "outputId": "6ef7d7b9-7045-4155-cc88-d2367d8198aa"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100000"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer = Embedding(max_features, 128)"
      ],
      "metadata": {
        "id": "BkjF8BFmqZR6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer(x_train[0]).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E730Dd9rqb7I",
        "outputId": "51c88017-4fc4-4362-b4f0-a0a6dd06a030"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([150, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Построение модели...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128))\n",
        "model.add(LSTM(128, dropout=0.1, recurrent_dropout=0.1))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZR1pBB1qdlE",
        "outputId": "2a3523fe-b6a4-4f01-b1bb-c159b330a698"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Построение модели...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "tZREZB0Wqiqx"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Процесс обучения...')\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=2, # увеличьте при необходимости\n",
        "          validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhJTXZ28qluU",
        "outputId": "988cd430-17d7-4b22-c506-c69c0c6c0ba7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Процесс обучения...\n",
            "Epoch 1/2\n",
            "84/84 [==============================] - 120s 1s/step - loss: 0.4552 - accuracy: 0.7701 - val_loss: 0.3400 - val_accuracy: 0.8618\n",
            "Epoch 2/2\n",
            "84/84 [==============================] - 117s 1s/step - loss: 0.1949 - accuracy: 0.9276 - val_loss: 0.3631 - val_accuracy: 0.8576\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a68d3539d20>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score, acc = model.evaluate(x_test, y_test,\n",
        "                            batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dz795KVLqqYT",
        "outputId": "285bf649-b66c-4358-bb5d-0f11c8735778"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "84/84 [==============================] - 19s 222ms/step - loss: 0.3631 - accuracy: 0.8576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Результат при тестировании:', score)\n",
        "print('Тестовая точность:', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmEhFBNOqu5I",
        "outputId": "a0a3b691-db57-45bd-9f54-a23a699141b0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Результат при тестировании: 0.36306464672088623\n",
            "Тестовая точность: 0.8575599789619446\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выводы:\n",
        "\n",
        "Удалось повысить точность модели на сете IMDB до 0,86 путём увеличения максимального количества признаков до 100 000.\n",
        "При этом был увеличен batch_size до 300.\n",
        "Также была увеличена max_len до 150.\n",
        "Количество эпох было увеличено до 2. Большее количество эпох ведёт к переобучению."
      ],
      "metadata": {
        "id": "SEfCkWzWqxva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 2.\n",
        "\n",
        "Попробуйте изменить параметры нейронной сети, генерирующей текст таким образом, чтобы добиться генерации как можно более осмысленного текста. Пришлите лучший текст из получившихся и опишите предпринятые для его получения действия. Можно использовать текст другого произведения."
      ],
      "metadata": {
        "id": "KvOfg9dyqy-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.layers import SimpleRNN, LSTM, GRU\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "metadata": {
        "id": "plJbxKntr3MZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "diGCnZ4Kr7Bz"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# построчное чтение из примера с текстом\n",
        "# with open(\"/content/drive/MyDrive/Colab Notebooks/13_5_files/13_5_python_wikipedia.txt\", 'rb') as _in:\n",
        "with open(\"13_5_files/13_5_python_wikipedia.txt\", 'rb') as _in:\n",
        "    lines = []\n",
        "    for line in _in:\n",
        "        line = line.strip().lower().decode(\"utf8\", \"ignore\")\n",
        "        if len(line) == 0:\n",
        "            continue\n",
        "        lines.append(line)\n",
        "text = \" \".join(lines)\n",
        "chars = set(text)\n",
        "nb_chars = len(chars)"
      ],
      "metadata": {
        "id": "LsODFb4tr9iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chars)"
      ],
      "metadata": {
        "id": "a3kTFE_msBep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# создание индекса символов и reverse mapping чтобы передвигаться между значениями numerical\n",
        "# ID and a specific character. The numerical ID will correspond to a column\n",
        "# ID и определенный символ. Numerical ID будет соответсвовать колонке\n",
        "# число при использовании one-hot кодировки для представление входов символов\n",
        "char2index = {c: i for i, c in enumerate(chars)}\n",
        "index2char = {i: c for i, c in enumerate(chars)}"
      ],
      "metadata": {
        "id": "CTi2SCeasDSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index2char"
      ],
      "metadata": {
        "id": "A5a3vdKusE3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# для удобства выберете фиксированную длину последовательность 10 символов\n",
        "SEQLEN, STEP = 100, 1\n",
        "input_chars, label_chars = [], []\n",
        "\n",
        "# конвертация data в серии разных SEQLEN-length субпоследовательностей\n",
        "for i in range(0, len(text) - SEQLEN, STEP):\n",
        "    input_chars.append(text[i: i + SEQLEN])\n",
        "    label_chars.append(text[i + SEQLEN])"
      ],
      "metadata": {
        "id": "6L0qTQowsHP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(input_chars)"
      ],
      "metadata": {
        "id": "sOAw4tu3sJ7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_chars[0], label_chars[0]"
      ],
      "metadata": {
        "id": "ZtiAwX-2sLzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_chars[1], label_chars[1]"
      ],
      "metadata": {
        "id": "daDqeNIzsN4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Вычисление one-hot encoding входных последовательностей X и следующего символа (the label) y\n",
        "\n",
        "X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\n",
        "y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)\n",
        "for i, input_char in enumerate(input_chars):\n",
        "    for j, ch in enumerate(input_char):\n",
        "        X[i, j, char2index[ch]] = 1\n",
        "    y[i, char2index[label_chars[i]]] = 1"
      ],
      "metadata": {
        "id": "u67ao5-osQBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "91799B6jsSwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# установка ряда метапамертров  для нейронной сети и процесса тренировки\n",
        "BATCH_SIZE, HIDDEN_SIZE = 256, 256\n",
        "NUM_ITERATIONS = 25 # 25 должно быть достаточно\n",
        "NUM_EPOCHS_PER_ITERATION = 2\n",
        "NUM_PREDS_PER_EPOCH = 100"
      ],
      "metadata": {
        "id": "vXGR7HNysUoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Создание очень простой рекуррентной нейронной сети. В ней будет один реккурентный закодированный входной слой.\n",
        "За ним последует полносвязный слой связанный с набором возможных следующих символов,\n",
        "которые конвертированы в вероятностные результаты через стандартную softmax активацию\n",
        "с multi-class cross-encoding loss функцию ссылающуются на предсказание one-hot encoding лейбл символа\n",
        "'''\n",
        "\n",
        "model = Sequential()\n",
        "model.add(\n",
        "    GRU(  # вы можете изменить эту часть на LSTM или SimpleRNN, чтобы попробовать альтернативы\n",
        "        HIDDEN_SIZE,\n",
        "        return_sequences=True,\n",
        "        input_shape=(SEQLEN, nb_chars),\n",
        "        unroll=True\n",
        "    )\n",
        ")\n",
        "\n",
        "model.add(\n",
        "    GRU(  # вы можете изменить эту часть на LSTM или SimpleRNN, чтобы попробовать альтернативы\n",
        "        HIDDEN_SIZE,\n",
        "        return_sequences=False,\n",
        "        unroll=True\n",
        "    )\n",
        ")\n",
        "\n",
        "model.add(Dense(nb_chars))\n",
        "model.add(Activation(\"softmax\"))\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
      ],
      "metadata": {
        "id": "hJ8UMCPWsXnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# выполнение серий тренировочных и демонстрационных итераций\n",
        "for iteration in range(NUM_ITERATIONS):\n",
        "\n",
        "    # для каждой итерации запуск передачи данных в модель\n",
        "    print(\"=\" * 50)\n",
        "    print(\"Итерация #: %d\" % (iteration))\n",
        "    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)\n",
        "\n",
        "    # Select a random example input sequence.\n",
        "    test_idx = np.random.randint(len(input_chars))\n",
        "    test_chars = input_chars[test_idx]\n",
        "\n",
        "    # для числа шагов предсказаний использование текущей тренируемой модели\n",
        "    # конструирование one-hot encoding для тестирования input и добавление предсказания.\n",
        "    print(\"Генерация из посева: %s\" % (test_chars))\n",
        "    print(test_chars, end=\"\")\n",
        "    for i in range(NUM_PREDS_PER_EPOCH):\n",
        "\n",
        "        # здесь one-hot encoding.\n",
        "        X_test = np.zeros((1, SEQLEN, nb_chars))\n",
        "        for j, ch in enumerate(test_chars):\n",
        "            X_test[0, j, char2index[ch]] = 1\n",
        "\n",
        "        # осуществление предсказания с помощью текущей модели.\n",
        "        pred = model.predict(X_test, verbose=0)[0]\n",
        "        y_pred = index2char[np.argmax(pred)]\n",
        "\n",
        "        # вывод предсказания добавленного к тестовому примеру\n",
        "        print(y_pred, end=\"\")\n",
        "\n",
        "        # инкрементация тестового примера содержащего предсказание\n",
        "        test_chars = test_chars[1:] + y_pred"
      ],
      "metadata": {
        "id": "t9pV3yLEsauU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выводы:\n",
        "\n",
        "Была использована статья из Википедии о ЯП Python\n",
        "Длина генерируемой последовательности (seqlen) была увеличена до 100.\n",
        "batch_size и hidden_size увеличены до 256.\n",
        "Количество итераций увеличено до 25, в каждой итерации 2 эпохи.\n",
        "Время обучения модели 42 мин 20 сек.\n",
        "Финальная ошибка составила 0,149."
      ],
      "metadata": {
        "id": "VNKQPbEmsfjH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 3"
      ],
      "metadata": {
        "id": "IMpDF_5Hskm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import sys\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "izKI5hZHsnwj"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Optimizer:\n",
        "    #USE SAME DEFAULTS AS KERAS ADAM OPTIMIZER\n",
        "    def __init__(self, lr=.1, beta_1=0.9, beta_2=0.999,\n",
        "                 epsilon=0, decay=0., **kwargs):\n",
        "\n",
        "        allowed_kwargs = {'clipnorm', 'clipvalue'}\n",
        "        for k in kwargs:\n",
        "            if k not in allowed_kwargs:\n",
        "                raise TypeError('Unexpected keyword argument '\n",
        "                                'passed to optimizer: ' + str(k))\n",
        "        self.__dict__.update(kwargs)\n",
        "        self.iterations = 1\n",
        "        self.lr = lr\n",
        "        self.beta_1 = beta_1\n",
        "        self.beta_2 = beta_2\n",
        "        self.decay = decay\n",
        "        self.epsilon = epsilon\n",
        "        self.initial_decay = decay\n",
        "\n",
        "    def get_ADAM(self, params, grads):\n",
        "\n",
        "        original_shapes = [x.shape for x in params]\n",
        "        params = [x.flatten() for x in params]\n",
        "        grads = [x.flatten() for x in grads]\n",
        "\n",
        "        lr = self.lr\n",
        "        if self.initial_decay > 0:\n",
        "            lr *= (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "        t = self.iterations + 1\n",
        "        lr_t = lr * (np.sqrt(1. - np.power(self.beta_2, t)) /\n",
        "                     (1. - np.power(self.beta_1, t)))\n",
        "\n",
        "        if not hasattr(self, 'ms'):\n",
        "            self.ms = [np.zeros(p.shape) for p in params]\n",
        "            self.vs = [np.zeros(p.shape) for p in params]\n",
        "\n",
        "        ret = [None] * len(params)\n",
        "        for i, p, g, m, v in zip(range(len(params)), params, grads, self.ms, self.vs):\n",
        "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
        "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * np.square(g)\n",
        "            p_t = p - lr_t * m_t / (np.sqrt(v_t) + self.epsilon)\n",
        "            self.ms[i] = m_t\n",
        "            self.vs[i] = v_t\n",
        "            ret[i] = p_t\n",
        "        self.iterations += 1\n",
        "\n",
        "        for i in range(len(ret)):\n",
        "            ret[i] = ret[i].reshape(original_shapes[i])\n",
        "\n",
        "        return np.array(ret)\n",
        "\n",
        "\n",
        "    def get_SGD(self, w,p):\n",
        "        for x,y in zip(w,p):\n",
        "                    x+=self.lr*y\n",
        "        return w[0],w[1],w[2],w[3],w[4],w[5],w[6],w[7],w[8],w[9]\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1. / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(values):\n",
        "    return values*(1-values)\n",
        "\n",
        "def tanh_derivative(values):\n",
        "    return 1. - values ** 2\n",
        "\n",
        "# createst uniform random array w/ values in [a,b) and shape args\n",
        "def rand_arr(a, b, *args):\n",
        "    np.random.seed(0)\n",
        "    return (np.random.rand(*args) * (b - a) + a)*.1\n",
        "\n",
        "class LstmParam:\n",
        "    def __init__(self, mem_cell_ct, x_dim,optimization):\n",
        "        self.mem_cell_ct = mem_cell_ct\n",
        "        self.x_dim = x_dim\n",
        "        concat_len = x_dim + mem_cell_ct\n",
        "\n",
        "        self.opt=Optimizer()\n",
        "        self.optimization=optimization\n",
        "\n",
        "        # weight matrices\n",
        "        self.wg = rand_arr(-0.1, 0.1, mem_cell_ct, concat_len)\n",
        "        self.wi = rand_arr(-0.1, 0.1, mem_cell_ct, concat_len)\n",
        "        self.wf = rand_arr(-0.1, 0.1, mem_cell_ct, concat_len)\n",
        "        self.wo = rand_arr(-0.1, 0.1, mem_cell_ct, concat_len)\n",
        "\n",
        "        # bias terms\n",
        "        self.bg = rand_arr(-0.1, 0.1, mem_cell_ct)\n",
        "        self.bi = rand_arr(-0.1, 0.1, mem_cell_ct)\n",
        "        self.bf = rand_arr(-0.1, 0.1, mem_cell_ct)\n",
        "        self.bo = rand_arr(-0.1, 0.1, mem_cell_ct)\n",
        "\n",
        "\n",
        "\n",
        "        # diffs (derivative of loss function w.r.t. all parameters)\n",
        "        self.wg_diff = np.zeros((mem_cell_ct, concat_len))\n",
        "        self.wi_diff = np.zeros((mem_cell_ct, concat_len))\n",
        "        self.wf_diff = np.zeros((mem_cell_ct, concat_len))\n",
        "        self.wo_diff = np.zeros((mem_cell_ct, concat_len))\n",
        "        self.bg_diff = np.zeros(mem_cell_ct)\n",
        "        self.bi_diff = np.zeros(mem_cell_ct)\n",
        "        self.bf_diff = np.zeros(mem_cell_ct)\n",
        "        self.bo_diff = np.zeros(mem_cell_ct)\n",
        "\n",
        "    def apply_diff(self, lr = .1):\n",
        "        if(self.optimization=='adam'):\n",
        "            self.wg=self.opt.get_ADAM(self.wg,self.wg_diff)\n",
        "            self.wi=self.opt.get_ADAM(np.array(self.wi),np.array(self.wi_diff))\n",
        "            self.wf=self.opt.get_ADAM(np.array(self.wf),np.array(self.wf_diff))\n",
        "            self.wo=self.opt.get_ADAM(np.array(self.wo),np.array(self.wo_diff))\n",
        "\n",
        "        else:\n",
        "            #This is the stochastic gradient descent code\n",
        "            self.wg -= lr * self.wg_diff\n",
        "            self.wi -= lr * self.wi_diff\n",
        "            self.wf -= lr * self.wf_diff\n",
        "            self.wo -= lr * self.wo_diff\n",
        "\n",
        "\n",
        "\n",
        "        self.bg -= lr * self.bg_diff\n",
        "        self.bi -= lr * self.bi_diff\n",
        "        self.bf -= lr * self.bf_diff\n",
        "        self.bo -= lr * self.bo_diff\n",
        "\n",
        "        # reset diffs to zero\n",
        "        self.wg_diff = np.zeros_like(self.wg)\n",
        "        self.wi_diff = np.zeros_like(self.wi)\n",
        "        self.wf_diff = np.zeros_like(self.wf)\n",
        "        self.wo_diff = np.zeros_like(self.wo)\n",
        "        self.bg_diff = np.zeros_like(self.bg)\n",
        "        self.bi_diff = np.zeros_like(self.bi)\n",
        "        self.bf_diff = np.zeros_like(self.bf)\n",
        "        self.bo_diff = np.zeros_like(self.bo)\n",
        "\n",
        "class LstmState:\n",
        "    def __init__(self, mem_cell_ct, x_dim):\n",
        "        self.g = np.zeros(mem_cell_ct)\n",
        "        self.i = np.zeros(mem_cell_ct)\n",
        "        self.f = np.zeros(mem_cell_ct)\n",
        "        self.o = np.zeros(mem_cell_ct)\n",
        "        self.s = np.zeros(mem_cell_ct)\n",
        "        self.h = np.zeros(mem_cell_ct)\n",
        "        self.bottom_diff_h = np.zeros_like(self.h)\n",
        "        self.bottom_diff_s = np.zeros_like(self.s)\n",
        "\n",
        "class LstmNode:\n",
        "    def __init__(self, lstm_param, lstm_state):\n",
        "        # store reference to parameters and to activations\n",
        "        self.state = lstm_state\n",
        "        self.param = lstm_param\n",
        "\n",
        "        # non-recurrent input concatenated with recurrent input\n",
        "        self.xc = None\n",
        "\n",
        "    def bottom_data_is(self, x, s_prev = None, h_prev = None):\n",
        "        # if this is the first lstm node in the network\n",
        "        if s_prev is None: s_prev = np.zeros_like(self.state.s)\n",
        "        if h_prev is None: h_prev = np.zeros_like(self.state.h)\n",
        "        # save data for use in backprop\n",
        "        self.s_prev = s_prev\n",
        "        self.h_prev = h_prev\n",
        "\n",
        "        # concatenate x(t) and h(t-1)\n",
        "        xc = np.hstack((x,  h_prev))\n",
        "        self.state.g = np.tanh(np.dot(self.param.wg, xc) + self.param.bg)\n",
        "        self.state.i = sigmoid(np.dot(self.param.wi, xc) + self.param.bi)\n",
        "        self.state.f = sigmoid(np.dot(self.param.wf, xc) + self.param.bf)\n",
        "        self.state.o = sigmoid(np.dot(self.param.wo, xc) + self.param.bo)\n",
        "        self.state.s = self.state.g * self.state.i + s_prev * self.state.f\n",
        "        self.state.h = self.state.s * self.state.o\n",
        "\n",
        "        self.xc = xc\n",
        "\n",
        "\n",
        "    def top_diff_is(self, top_diff_h, top_diff_s):\n",
        "        # notice that top_diff_s is carried along the constant error carousel\n",
        "        ds = self.state.o * top_diff_h + top_diff_s\n",
        "        do = self.state.s * top_diff_h\n",
        "        di = self.state.g * ds\n",
        "        dg = self.state.i * ds\n",
        "        df = self.s_prev * ds\n",
        "\n",
        "        # diffs w.r.t. vector inside sigma / tanh function\n",
        "        di_input = sigmoid_derivative(self.state.i) * di\n",
        "        df_input = sigmoid_derivative(self.state.f) * df\n",
        "        do_input = sigmoid_derivative(self.state.o) * do\n",
        "        dg_input = tanh_derivative(self.state.g) * dg\n",
        "\n",
        "        # diffs w.r.t. inputs\n",
        "        self.param.wi_diff += np.outer(di_input, self.xc)\n",
        "        self.param.wf_diff += np.outer(df_input, self.xc)\n",
        "        self.param.wo_diff += np.outer(do_input, self.xc)\n",
        "        self.param.wg_diff += np.outer(dg_input, self.xc)\n",
        "        self.param.bi_diff += di_input\n",
        "        self.param.bf_diff += df_input\n",
        "        self.param.bo_diff += do_input\n",
        "        self.param.bg_diff += dg_input\n",
        "\n",
        "        #for dparam in [self.param.wi_diff, self.param.wf_diff , self.param.wo_diff, self.param.wg_diff, self.param.bi_diff, self.param.bf_diff, self.param.bo_diff, self.param.bg_diff]:\n",
        "        #    np.clip(dparam, -1, 1, out=dparam)\n",
        "\n",
        "        # compute bottom diff\n",
        "        dxc = np.zeros_like(self.xc)\n",
        "        dxc += np.dot(self.param.wi.T, di_input)\n",
        "        dxc += np.dot(self.param.wf.T, df_input)\n",
        "        dxc += np.dot(self.param.wo.T, do_input)\n",
        "        dxc += np.dot(self.param.wg.T, dg_input)\n",
        "\n",
        "        # save bottom diffs\n",
        "        self.state.bottom_diff_s = ds * self.state.f\n",
        "        self.state.bottom_diff_h = dxc[self.param.x_dim:]\n",
        "\n",
        "class LstmNetwork():\n",
        "    def __init__(self, lstm_param, loss):\n",
        "        self.lstm_param = lstm_param\n",
        "        self.lstm_node_list = []\n",
        "        # input sequence\n",
        "        self.x_list = []\n",
        "        self.loss=loss\n",
        "\n",
        "    def y_list_is(self, y_list, loss_layer):\n",
        "        \"\"\"\n",
        "        Updates diffs by setting target sequence\n",
        "        with corresponding loss layer.\n",
        "        Will *NOT* update parameters.  To update parameters,\n",
        "        call self.lstm_param.apply_diff()\n",
        "        \"\"\"\n",
        "        assert len(y_list) == len(self.x_list)\n",
        "        idx = len(self.x_list) - 1\n",
        "        # first node only gets diffs from label ...\n",
        "        loss = loss_layer.loss(self.lstm_node_list[idx].state.h, y_list[idx],self.loss)\n",
        "\n",
        "        diff_h =loss_layer.bottom_diff(self.lstm_node_list[idx].state.h, y_list[idx])\n",
        "\n",
        "        # here s is not affecting loss due to h(t+1), hence we set equal to zero\n",
        "        diff_s = np.zeros(self.lstm_param.mem_cell_ct)\n",
        "        self.lstm_node_list[idx].top_diff_is(diff_h, diff_s)\n",
        "        idx -= 1\n",
        "\n",
        "        ### ... following nodes also get diffs from next nodes, hence we add diffs to diff_h\n",
        "        ### we also propagate error along constant error carousel using diff_s\n",
        "        while idx >= 0:\n",
        "            loss += loss_layer.loss(self.lstm_node_list[idx].state.h, y_list[idx],self.loss)\n",
        "            diff_h = loss_layer.bottom_diff(self.lstm_node_list[idx].state.h, y_list[idx])\n",
        "            diff_h += self.lstm_node_list[idx + 1].state.bottom_diff_h\n",
        "            diff_s = self.lstm_node_list[idx + 1].state.bottom_diff_s\n",
        "            self.lstm_node_list[idx].top_diff_is(diff_h, diff_s)\n",
        "            idx -= 1\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def x_list_clear(self):\n",
        "        self.x_list = []\n",
        "\n",
        "    def x_list_add(self, x):\n",
        "        self.x_list.append(x)\n",
        "       # print(self.x_list)\n",
        "        if len(self.x_list) > len(self.lstm_node_list):\n",
        "            # need to add new lstm node, create new state mem\n",
        "            lstm_state = LstmState(self.lstm_param.mem_cell_ct, self.lstm_param.x_dim)\n",
        "            self.lstm_node_list.append(LstmNode(self.lstm_param, lstm_state))\n",
        "\n",
        "        # get index of most recent x input\n",
        "        idx = len(self.x_list) - 1\n",
        "        if idx == 0:\n",
        "            # no recurrent inputs yet\n",
        "            self.lstm_node_list[idx].bottom_data_is(x)\n",
        "        else:\n",
        "            s_prev = self.lstm_node_list[idx - 1].state.s\n",
        "            h_prev = self.lstm_node_list[idx - 1].state.h\n",
        "            self.lstm_node_list[idx].bottom_data_is(x, s_prev, h_prev)\n",
        "\n",
        "\n",
        "\n",
        "class LossLayer:\n",
        "    \"\"\"\n",
        "    Computes square loss with first element of hidden layer array.\n",
        "    MG-Attempted to add in mae loss for comparison, but RMSE and MAE loss performed the same.\n",
        "    \"\"\"\n",
        "    @classmethod\n",
        "    def loss(self,pred, label,fn):\n",
        "        if(fn=='mae'):\n",
        "            return LossLayer.loss_mae(pred,label)\n",
        "        else:\n",
        "            return LossLayer.loss_rmse(pred,label)\n",
        "\n",
        "    # MG added mean absolute error\n",
        "    @classmethod\n",
        "    def loss_mae(self, pred, label):\n",
        "        return (np.abs(pred[0]-label))\n",
        "        #return (pred[0] - label) ** 2\n",
        "\n",
        "    @classmethod\n",
        "    def loss_rmse(self, pred, label):\n",
        "        return (pred[0] - label) ** 2\n",
        "\n",
        "    @classmethod\n",
        "    def bottom_diff(self, pred, label):\n",
        "        diff = np.zeros_like(pred)\n",
        "        diff[0] =2*(pred[0] - label)\n",
        "        return diff\n",
        "\n",
        "\n",
        "\n",
        "def train(loss, optimization):\n",
        "    mem_cell_ct = 50\n",
        "    x_dim = 4\n",
        "    lstm_param = LstmParam(mem_cell_ct, x_dim,optimization)\n",
        "    lstm_net = LstmNetwork(lstm_param,loss)\n",
        "    losses=[]\n",
        "    bestLoss=1e5\n",
        "    print(\"Training...\")\n",
        "    for cur_iter in range(100):\n",
        "\n",
        "        for ind in range(len(Y)):\n",
        "            lstm_net.x_list_add(X[ind])\n",
        "\n",
        "        if(cur_iter%50==0):\n",
        "            print(\"iter\", \"%2s\" % str(cur_iter), end=\": \")\n",
        "            print(\"y_pred = [\" +\n",
        "                  \", \".join([\"% 2.5f\" % lstm_net.lstm_node_list[ind].state.h[0] for ind in range(len(Y))]) +\n",
        "                  \"]\", end=\", \")\n",
        "\n",
        "        loss = lstm_net.y_list_is(Y, LossLayer)\n",
        "        losses.append(loss)\n",
        "        if(loss<bestLoss):\n",
        "            best_lstm_net = LstmNetwork(lstm_param,loss)\n",
        "\n",
        "        lstm_param.apply_diff(lr=0.1)\n",
        "\n",
        "        if(cur_iter%50==0):\n",
        "            print(\"loss:\", \"%.3e\" % loss)\n",
        "\n",
        "        lstm_net.x_list_clear()\n",
        "\n",
        "    for ind in range(len(Y)):\n",
        "        best_lstm_net.x_list_add(X[ind])\n",
        "    loss = best_lstm_net.y_list_is(Y, LossLayer)\n",
        "    return losses, [ best_lstm_net.lstm_node_list[ind].state.h[0] for ind in range(len(Y))],loss"
      ],
      "metadata": {
        "id": "_15NR_8UsvrB"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pd.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txOV9S9JszaV",
        "outputId": "361ffcae-4691-47c7-feb1-6c81ade11e28"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9Dkl3Fa6s3Dr"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def firstTurbineData():\n",
        "    df = pd.read_csv('13_5_files/13_5_la-haute-borne-data-2013-2016.zip', compression='zip', sep=';')\n",
        "#     df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/13_5_files/13_5_la-haute-borne-data-2013-2016.zip', compression='zip', sep=';')\n",
        "    df['Date_time'] = df['Date_time'].astype(str).str[:-6] #remove timezone (caused me an hour of pain)\n",
        "    df.Date_time=pd.to_datetime(df['Date_time'])\n",
        "    df=df.fillna(method='ffill')\n",
        "\n",
        "    df=df.sort_values(by='Date_time')\n",
        "    df = df.reset_index()\n",
        "    turbines=df.Wind_turbine_name.unique()\n",
        "    print(\"Turbine name: \"+str(turbines[0]))\n",
        "    turbineData=df[df['Wind_turbine_name']==turbines[0]]\n",
        "    return turbineData"
      ],
      "metadata": {
        "id": "QbSDAKIzs5hz"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createGraph(losses, title):\n",
        "    X = np.arange(0,len(losses))\n",
        "    figure = plt.figure()\n",
        "    tick_plot = figure.add_subplot(1, 1, 1)\n",
        "    tick_plot.plot(X, losses,  color='green', linestyle='-', marker='*' )\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ITpMxsubs8cg"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "date_to_test=datetime.datetime(2016, 1, 1)\n",
        "turbineData=np.sin(firstTurbineData().Wa_c_avg.values)[:10]\n",
        "X=np.array([turbineData[:4],\n",
        "                   turbineData[1:5],\n",
        "                   turbineData[2:6],\n",
        "                   turbineData[3:7],\n",
        "                   turbineData[4:8],\n",
        "                   turbineData[5:9]])\n",
        "Y=np.array([turbineData[4],\n",
        "                   turbineData[5],\n",
        "                   turbineData[6],\n",
        "                   turbineData[7],\n",
        "                   turbineData[8],\n",
        "                   turbineData[9]])"
      ],
      "metadata": {
        "id": "mCut3PIms-1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses, predictions,loss=train('rmse','sgd')\n",
        "print(\"Actual vs Predicted:\")\n",
        "print(Y)\n",
        "print(predictions)\n",
        "createGraph(losses,\"SGD Optimization\\nLoss=\"+str(loss))\n",
        "losses, predictions,loss=train('rmse','adam')\n",
        "print(\"Actual vs Predicted:\")\n",
        "print(Y)\n",
        "print(predictions)\n",
        "createGraph(losses,\"Adam Optimization\\nLoss=\"+str(loss))"
      ],
      "metadata": {
        "id": "waaxy2GRtCnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 4."
      ],
      "metadata": {
        "id": "GZXvfYsstFp0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Предложите свои варианты решения проблемы исчезающего градиента в RNN.\n",
        "\n",
        "Использование других функций активации (т. н. выпрямители):\n",
        "GeLU - Линейная единица измерения ошибки Гаусса\n",
        "SiLU - Sigmoid Linear Unit\n",
        "softplus\n",
        "Параметрическое ReLU\n",
        "Более быстрое оборудование\n",
        "Остаточные нейронные сети\n",
        "Применение алгоритмов не основанных на градиенте"
      ],
      "metadata": {
        "id": "Am6T61CjtNKM"
      }
    }
  ]
}