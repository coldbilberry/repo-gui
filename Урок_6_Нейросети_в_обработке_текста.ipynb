{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3Q69DxuI9rHjbbcYaRLrm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coldbilberry/repo-gui/blob/main/%D0%A3%D1%80%D0%BE%D0%BA_6_%D0%9D%D0%B5%D0%B9%D1%80%D0%BE%D1%81%D0%B5%D1%82%D0%B8_%D0%B2_%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B5_%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gJGH0eb5F91"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_words = 2000\n",
        "max_len = 10\n",
        "num_classes = 1\n",
        "\n",
        "epochs = 25\n",
        "batch_size = 512\n",
        "print_batch_n = 100"
      ],
      "metadata": {
        "id": "DxdGeOoe5WQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rYg8AKQv7PYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_csv = '/content/drive/My Drive/Twitter Sentiment Analysis/train.csv'\n",
        "test_csv = '/content/drive/My Drive/Twitter Sentiment Analysis/test.csv'"
      ],
      "metadata": {
        "id": "IYLsm47r7TDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(train_csv)\n",
        "\n",
        "df_train , df_val = df[:25000].copy(), df[25001:].copy()\n",
        "\n",
        "df_test = pd.read_csv(test_csv)"
      ],
      "metadata": {
        "id": "PjgiRa6v7Wzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "id": "gVxl5MQO7YvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['label'].value_counts()"
      ],
      "metadata": {
        "id": "IYS71It-7bSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_val['label'].value_counts()"
      ],
      "metadata": {
        "id": "alqxhqH-7cCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stop-words pymorphy2"
      ],
      "metadata": {
        "id": "1J7h5pn57eEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "from stop_words import get_stop_words\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "import re"
      ],
      "metadata": {
        "id": "eJxctoJo7gpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sw = set(get_stop_words(\"en\"))\n",
        "sw"
      ],
      "metadata": {
        "id": "lrc72ZSP7jLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "puncts = set(punctuation)\n",
        "puncts"
      ],
      "metadata": {
        "id": "5vDxfplW7lGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "morpher = MorphAnalyzer()"
      ],
      "metadata": {
        "id": "LvM_S_kA7oZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(txt):\n",
        "    txt = str(txt)\n",
        "    txt = \"\".join(c for c in txt if c not in puncts)\n",
        "    txt = txt.lower()\n",
        "    txt = re.sub(\"не\\s\", \"не\", txt)\n",
        "    txt = [morpher.parse(word)[0].normal_form for word in txt.split() if word not in sw]\n",
        "    return \" \".join(txt)"
      ],
      "metadata": {
        "id": "XLZdPSu87sDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['tweet'].iloc[:1].values"
      ],
      "metadata": {
        "id": "3BKHOdvt7tQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['tweet'].iloc[:1].apply(preprocess_text).values"
      ],
      "metadata": {
        "id": "jdJtr27L7vDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "df_train['tweet'] = df_train['tweet'].progress_apply(preprocess_text)\n",
        "df_val['tweet'] = df_val['tweet'].progress_apply(preprocess_text)"
      ],
      "metadata": {
        "id": "RiXZkKmh7yXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_corpus = \" \".join(df_train[\"tweet\"])\n",
        "train_corpus = train_corpus.lower()"
      ],
      "metadata": {
        "id": "vpb9KxU970t9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "tokens = word_tokenize(train_corpus)\n",
        "tokens[:5]"
      ],
      "metadata": {
        "id": "9cEa-Mc472-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_filtered = [word for word in tokens if word.isalnum()]"
      ],
      "metadata": {
        "id": "r5d8XFhD75uR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "\n",
        "dist = FreqDist(tokens_filtered)\n",
        "tokens_filtered_top = [pair[0] for pair in dist.most_common(max_words-1)]  # вычитание 1 для padding\n",
        "len(tokens_filtered_top)"
      ],
      "metadata": {
        "id": "CgYEZn7s77zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_filtered_top[:10]"
      ],
      "metadata": {
        "id": "h_vYYXQ77-1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = {v: k for k, v in dict(enumerate(tokens_filtered_top, 1)).items()}\n",
        "vocabulary"
      ],
      "metadata": {
        "id": "vhXH2ikZ8AhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_sequence(text, maxlen):\n",
        "    result = []\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens_filtered = [word for word in tokens if word.isalnum()]\n",
        "    for word in tokens_filtered:\n",
        "        if word in vocabulary:\n",
        "            result.append(vocabulary[word])\n",
        "\n",
        "    padding = [0] * (maxlen-len(result))\n",
        "    return result[-maxlen:] + padding"
      ],
      "metadata": {
        "id": "yQ0CsXje8C1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "x_train = np.asarray([text_to_sequence(text, max_len) for text in df_train[\"tweet\"]])\n",
        "x_val = np.asarray([text_to_sequence(text, max_len) for text in df_val[\"tweet\"]])"
      ],
      "metadata": {
        "id": "mpJ_0VJ08IVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "id": "ZV4w-ASo8MKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['tweet'].iloc[0]"
      ],
      "metadata": {
        "id": "er8ol99t8RO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[0]"
      ],
      "metadata": {
        "id": "d2zEsQfx8Td7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, vocab_size=2000, embedding_dim=128, out_channel=128, num_classes=1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.conv_1 = nn.Conv1d(embedding_dim, out_channel, kernel_size=2)\n",
        "        self.conv_2 = nn.Conv1d(embedding_dim, out_channel, kernel_size=3)\n",
        "        self.pool = nn.MaxPool1d(2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear_1 = nn.Linear(out_channel, out_channel // 2)\n",
        "        self.linear_2 = nn.Linear(out_channel // 2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.embedding(x) # B, L, E\n",
        "        #                       B  E  L\n",
        "        output = output.permute(0, 2, 1)\n",
        "        output = self.conv_1(output)\n",
        "        output = self.relu(output)\n",
        "        output = self.pool(output)\n",
        "\n",
        "        output = self.conv_2(output)\n",
        "        output = self.relu(output)\n",
        "        output = self.pool(output)\n",
        "        output = torch.max(output, axis=2).values\n",
        "        output = self.linear_1(output)\n",
        "        output = self.relu(output)\n",
        "        output = self.linear_2(output)\n",
        "        output = F.sigmoid(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "AdLsi_wv8VMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "class DataWrapper(Dataset):\n",
        "    def __init__(self, data, target, transform=None):\n",
        "        self.data = torch.from_numpy(data).long()\n",
        "        self.target = torch.from_numpy(target).long()\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.target[index]\n",
        "\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "id": "Tl3fd4w78YMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = DataWrapper(x_train, df_train['label'].values)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_dataset = DataWrapper(x_val, df_val['label'].values)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)"
      ],
      "metadata": {
        "id": "Es4T4JXC8a9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, l in train_loader:\n",
        "    print(x.shape)\n",
        "    print(l.shape)\n",
        "    print(l[0])\n",
        "    break"
      ],
      "metadata": {
        "id": "3C-bD0_L8c4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net(vocab_size=max_words)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "8mSo6r2h8ejq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)\n",
        "print(\"Parameters:\", sum([param.nelement() for param in model.parameters()]))"
      ],
      "metadata": {
        "id": "Gj919C_e8gyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.BCELoss()"
      ],
      "metadata": {
        "id": "WjtH8njB8lh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)\n",
        "model.train()\n",
        "th = 0.4\n",
        "\n",
        "train_loss_history = []\n",
        "test_loss_history = []\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    running_items, running_right = 0.0, 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels.float().view(-1, 1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss = loss.item()\n",
        "        running_items += len(labels)\n",
        "\n",
        "        pred_labels = torch.squeeze((outputs > th).int())\n",
        "        running_right += (labels == pred_labels).sum()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}]. ' \\\n",
        "            f'Step [{i + 1}/{len(train_loader)}]. ' \\\n",
        "            f'Loss: {loss:.3f}. ' \\\n",
        "            f'Acc: {running_right / running_items:.3f}', end='. ')\n",
        "    running_loss, running_items, running_right = 0.0, 0.0, 0.0\n",
        "    train_loss_history.append(loss)\n",
        "\n",
        "    test_running_right, test_running_total, test_loss = 0.0, 0.0, 0.0\n",
        "    for j, data in enumerate(val_loader):\n",
        "        test_labels = data[1].to(device)\n",
        "        test_outputs = model(data[0].to(device))\n",
        "\n",
        "        test_loss = criterion(test_outputs, test_labels.float().view(-1, 1))\n",
        "\n",
        "        test_running_total += len(data[1])\n",
        "        pred_test_labels = torch.squeeze((test_outputs > th).int())\n",
        "        test_running_right += (test_labels == pred_test_labels).sum()\n",
        "\n",
        "    test_loss_history.append(test_loss.item())\n",
        "    print(f'Test loss: {test_loss:.3f}. Test acc: {test_running_right / test_running_total:.3f}')\n",
        "\n",
        "    model.train()\n",
        "\n",
        "print('Training is finished!')"
      ],
      "metadata": {
        "id": "MU-1khjf8mEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.title('Loss history')\n",
        "plt.grid(True)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.plot(train_loss_history, label='train')\n",
        "plt.plot(test_loss_history, label='test')\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "Oha7CLQD8zM6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}