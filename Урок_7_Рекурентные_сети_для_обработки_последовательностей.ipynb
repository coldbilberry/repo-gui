{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfwhpjvu78sSh1yAe0gnVO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coldbilberry/repo-gui/blob/main/%D0%A3%D1%80%D0%BE%D0%BA_7_%D0%A0%D0%B5%D0%BA%D1%83%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B5_%D1%81%D0%B5%D1%82%D0%B8_%D0%B4%D0%BB%D1%8F_%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B8_%D0%BF%D0%BE%D1%81%D0%BB%D0%B5%D0%B4%D0%BE%D0%B2%D0%B0%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D0%BE%D1%81%D1%82%D0%B5%D0%B9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Dzs4mkvM-I6P"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_words = 2000\n",
        "max_len = 10\n",
        "num_classes = 1\n",
        "\n",
        "# Training\n",
        "\n",
        "epochs = 25\n",
        "batch_size = 512\n",
        "print_batch_n = 100"
      ],
      "metadata": {
        "id": "kXDtsRY5-ja9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "train_csv = '/content/drive/My Drive/Twitter Sentiment Analysis/train.csv'\n",
        "test_csv = '/content/drive/My Drive/Twitter Sentiment Analysis/test.csv'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FhoIDLA-m13",
        "outputId": "46716a51-72c1-4764-eef6-2f0942866fe2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(train_csv)\n",
        "df_train , df_val = df[:25000].copy(), df[25001:].copy()\n",
        "df_test = pd.read_csv(test_csv)\n",
        "\n",
        "df_train.head()"
      ],
      "metadata": {
        "id": "3B-yF2JU-xaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stop-words pymorphy2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpgeqWG-_SDn",
        "outputId": "0d1153a7-5799-4f84-a07b-4654d9565bf1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stop-words\n",
            "  Downloading stop-words-2018.7.23.tar.gz (31 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dawg-python>=0.7.1 (from pymorphy2)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2)\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6 (from pymorphy2)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: stop-words, docopt\n",
            "  Building wheel for stop-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stop-words: filename=stop_words-2018.7.23-py3-none-any.whl size=32894 sha256=1057086c5781675783d1ab074ec1ecf560d5c2e55bf33dee189361161f48a9ca\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/1a/23/f12552a50cb09bcc1694a5ebb6c2cd5f2a0311de2b8c3d9a89\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=dd8a2f95fd210d292eee0d9b02ee117b95e72ad1756775cc2815a06f3f8e24b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built stop-words docopt\n",
            "Installing collected packages: stop-words, pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 stop-words-2018.7.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "from stop_words import get_stop_words\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re"
      ],
      "metadata": {
        "id": "QfUkPaj2_X5D"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sw = set(get_stop_words(\"en\"))\n",
        "puncts = set(punctuation)"
      ],
      "metadata": {
        "id": "RFovJX2T_bJY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "morpher = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3GDVrDi_cR-",
        "outputId": "66d4300b-9285-490f-938f-2813fc03f69d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(txt):\n",
        "    txt = str(txt)\n",
        "    txt = \"\".join(c for c in txt if c not in puncts)\n",
        "    txt = txt.lower()\n",
        "    txt = re.sub(\"не\\s\", \"не\", txt)\n",
        "    txt = [morpher.lemmatize(word) for word in txt.split() if word not in sw]\n",
        "    return \" \".join(txt)"
      ],
      "metadata": {
        "id": "Jg-M_1Xw_gd5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "df_train['tweet'] = df_train['tweet'].progress_apply(preprocess_text)\n",
        "df_val['tweet'] = df_val['tweet'].progress_apply(preprocess_text)"
      ],
      "metadata": {
        "id": "W8HML9HC_jeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_corpus = \" \".join(df_train[\"tweet\"])\n",
        "train_corpus = train_corpus.lower()\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "tokens = word_tokenize(train_corpus)\n",
        "tokens[:5]"
      ],
      "metadata": {
        "id": "P8eirrvD_nW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "\n",
        "tokens_filtered = [word for word in tokens if word.isalnum()]\n",
        "\n",
        "dist = FreqDist(tokens_filtered)\n",
        "tokens_filtered_top = [pair[0] for pair in dist.most_common(max_words-1)]  # вычитание 1 для padding\n",
        "len(tokens_filtered_top)"
      ],
      "metadata": {
        "id": "1a0ddwa8_p9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = {v: k for k, v in dict(enumerate(tokens_filtered_top, 1)).items()}"
      ],
      "metadata": {
        "id": "Dg7XbC1b_sDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_sequence(text, maxlen):\n",
        "    result = []\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens_filtered = [word for word in tokens if word.isalnum()]\n",
        "    for word in tokens_filtered:\n",
        "        if word in vocabulary:\n",
        "            result.append(vocabulary[word])\n",
        "\n",
        "    padding = [0] * (maxlen-len(result))\n",
        "    return result[-maxlen:] + padding"
      ],
      "metadata": {
        "id": "Z-Vd1_iN_viF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "x_train = np.asarray([text_to_sequence(text, max_len) for text in df_train[\"tweet\"]])\n",
        "x_val = np.asarray([text_to_sequence(text, max_len) for text in df_val[\"tweet\"]])"
      ],
      "metadata": {
        "id": "EZ96f7cs_zab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class GRUFixedLen(nn.Module) :\n",
        "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=128, use_last=True):\n",
        "        super().__init__()\n",
        "        self.use_last = use_last\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=2, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, 1)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embeddings(x)\n",
        "        x = self.dropout(x)\n",
        "        gru_out, ht = self.gru(x)\n",
        "\n",
        "        if self.use_last:\n",
        "            last_tensor = gru_out[:,-1,:]\n",
        "        else:\n",
        "            # use mean\n",
        "            last_tensor = torch.mean(gru_out[:,:], dim=1)\n",
        "\n",
        "        out = self.linear(last_tensor)\n",
        "        return torch.sigmoid(out)"
      ],
      "metadata": {
        "id": "9zTKerHl_00X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class DataWrapper(Dataset):\n",
        "    def __init__(self, data, target, transform=None):\n",
        "        self.data = torch.from_numpy(data).long()\n",
        "        self.target = torch.from_numpy(target).long()\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.target[index]\n",
        "\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "id": "148eNLQr_3Ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = DataWrapper(x_train, df_train['label'].values)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_dataset = DataWrapper(x_val, df_val['label'].values)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "for x, l in train_loader:\n",
        "    print(x.shape)\n",
        "    print(l.shape)\n",
        "    print(l[0])\n",
        "    break"
      ],
      "metadata": {
        "id": "wWrjvfsX_52W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gru_init = GRUFixedLen(max_words, 128, 20, use_last=False)\n",
        "optimizer = torch.optim.Adam(gru_init.parameters(), lr=0.001)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "print(gru_init)\n",
        "print(\"Parameters:\", sum([param.nelement() for param in gru_init.parameters()]))"
      ],
      "metadata": {
        "id": "5l1pidFz__XC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "device"
      ],
      "metadata": {
        "id": "E48bNVTfABUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gru_init = gru_init.to(device)\n",
        "gru_init.train()\n",
        "th = 0.4\n",
        "\n",
        "train_loss_history = []\n",
        "test_loss_history = []\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    gru_init.train()\n",
        "    running_items, running_right = 0.0, 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = gru_init(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels.float().view(-1, 1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss = loss.item()\n",
        "        running_items += len(labels)\n",
        "\n",
        "        pred_labels = torch.squeeze((outputs > th).int())\n",
        "        running_right += (labels == pred_labels).sum()\n",
        "\n",
        "    gru_init.eval()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}]. ' \\\n",
        "          f'Step [{i + 1}/{len(train_loader)}]. ' \\\n",
        "          f'Loss: {loss:.3f}. ' \\\n",
        "          f'Acc: {running_right / running_items:.3f}', end='. ')\n",
        "    running_loss, running_items, running_right = 0.0, 0.0, 0.0\n",
        "    train_loss_history.append(loss)\n",
        "\n",
        "    test_running_right, test_running_total, test_loss = 0.0, 0.0, 0.0\n",
        "    for j, data in enumerate(val_loader):\n",
        "        test_labels = data[1].to(device)\n",
        "        test_outputs = gru_init(data[0].to(device))\n",
        "\n",
        "        test_loss = criterion(test_outputs, test_labels.float().view(-1, 1))\n",
        "\n",
        "        test_running_total += len(data[1])\n",
        "        pred_test_labels = torch.squeeze((test_outputs > th).int())\n",
        "        test_running_right += (test_labels == pred_test_labels).sum()\n",
        "\n",
        "    test_loss_history.append(test_loss.item())\n",
        "    print(f'Test loss: {test_loss:.3f}. Test acc: {test_running_right / test_running_total:.3f}')\n",
        "\n",
        "print('Training is finished!')"
      ],
      "metadata": {
        "id": "uDyXEGA7AEWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.title('Loss history')\n",
        "plt.grid(True)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.plot(train_loss_history, label='train')\n",
        "plt.plot(test_loss_history, label='test')\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "QbPq1eaEARdo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}